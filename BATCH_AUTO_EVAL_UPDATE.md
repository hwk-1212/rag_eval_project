# 批量自动评估功能更新

## 📅 更新日期
2025-10-13

## 🎯 功能概述

本次更新将自动评估功能改造为**批量评估模式**，并优化了统计对比界面，实现了真正的多RAG技术对比评估体系。

## 🚀 核心改进

### 1. **评估配置面板** (sidebar)
- ✅ 在侧边栏LLM配置下方新增「🤖 自动评估配置」
- ✅ 支持开关"查询后自动评估"（默认开启）
- ✅ 支持开关"使用Ragas评估"（可选）
- ✅ 显示评估维度说明

**位置**: 侧边栏 → LLM配置下方

**配置项**:
```
🤖 自动评估配置
├── ☑️ 查询后自动评估
├── ☑️ 使用Ragas评估
└── 📊 评估维度说明
```

---

### 2. **批量自动评估** (rag_comparison)

#### 功能结构
```
📊 RAG结果对比
│
├── 🤖 自动评估 (上半部分)
│   ├── 🚀 批量评估所有RAG技术 (按钮)
│   ├── 各RAG技术详情 (标签页)
│   │   ├── 答案内容
│   │   ├── 执行时间 & 综合得分
│   │   ├── 检索文档 (折叠)
│   │   └── 详细评估结果 (折叠)
│   │       ├── LLM评分 (6维度)
│   │       └── Ragas评分 (2指标)
│   └── 评估状态显示
│
└── 📈 统计对比 (下半部分)
    ├── 📊 详细对比表
    ├── 📊 可视化对比
    │   ├── LLM评分对比 (柱状图)
    │   ├── Ragas对比 (柱状图)
    │   └── 性能对比 (柱状图)
    └── 💡 推荐 (Top 3)
```

#### 操作流程
1. **提交查询**: 在对话窗口输入问题，选择多个RAG技术
2. **查看结果**: 右侧显示各RAG技术的答案和执行时间
3. **批量评估**: 点击「🚀 批量评估所有RAG技术」按钮
4. **进度显示**: 实时显示评估进度 (1/8, 2/8, ...)
5. **查看对比**: 自动跳转到统计对比面板

---

### 3. **Ragas评估优化**

#### 问题
原版Ragas的部分指标需要`ground_truth`（标准答案），但本项目无测试集。

#### 解决方案
只使用**不需要标准答案**的Ragas指标：
- ✅ `faithfulness` (忠实度): 答案是否基于检索上下文
- ✅ `answer_relevancy` (答案相关性): 答案是否回答了问题

❌ 移除需要标准答案的指标：
- `context_precision` (需要ground_truth)
- `context_recall` (需要ground_truth)
- `answer_similarity` (需要ground_truth)
- `answer_correctness` (需要ground_truth)

**修改文件**: `backend/core/ragas_evaluator.py`

---

### 4. **统计对比面板**

#### 详细对比表
显示所有RAG技术的全量指标：

| RAG技术 | 执行时间 | 相关性 | 忠实度 | 连贯性 | 流畅度 | 简洁性 | 综合得分 | Ragas-Faithfulness | Ragas-Answer_Rel | 检索精确度 | 上下文相关性 |
|---------|----------|--------|--------|--------|--------|--------|----------|-------------------|------------------|------------|--------------|
| Simple RAG | 2.35 | 8.5 | 9.0 | 8.0 | 8.5 | 7.5 | 8.3 | 0.845 | 0.923 | 0.85 | 8.2 |
| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |

#### 可视化对比
1. **LLM评分对比**: 6维度柱状图 (相关性、忠实度、连贯性、流畅度、简洁性、综合)
2. **Ragas对比**: 2指标柱状图 (Faithfulness、Answer Relevancy)
3. **性能对比**: 执行时间 + 检索质量

#### 推荐系统
自动分析并推荐：
- 🏆 **综合得分Top 3**
- ⚡ **最快RAG技术**
- 🎯 **最优检索质量**

---

## 📊 评估指标体系

### LLM评分 (0-10分)
| 指标 | 说明 |
|------|------|
| 相关性 | 答案是否直接回答问题 |
| 忠实度 | 答案是否基于检索内容，无幻觉 |
| 连贯性 | 逻辑是否清晰 |
| 流畅度 | 语言是否自然 |
| 简洁性 | 表达是否简洁 |
| 综合得分 | 加权总分 |

### Ragas评分 (0-1)
| 指标 | 说明 |
|------|------|
| Faithfulness | 答案对检索内容的忠实度 |
| Answer Relevancy | 答案对问题的相关性 |

### 检索质量
| 指标 | 说明 |
|------|------|
| 检索精确度 | 检索文档的相关性分数 |
| 上下文相关性 | 上下文与问题的匹配度 |

---

## 🎬 使用示例

### 场景1: 快速对比
1. 输入问题: "什么是RAG？"
2. 选择RAG技术: Simple RAG, Reranker RAG, Fusion RAG
3. 点击「批量评估」
4. 查看统计对比，找到综合得分最高的技术

### 场景2: 深度评估
1. 输入复杂问题
2. 选择所有8种RAG技术
3. 启用Ragas评估
4. 批量评估后，分析各维度指标
5. 根据推荐选择最佳RAG策略

---

## 🔧 技术实现

### 前端 (`frontend/components/rag_comparison.py`)
- **批量评估函数** `batch_evaluate_all()`: 遍历所有RAG结果，逐个调用评估API
- **进度条**: 实时显示评估进度
- **数据汇总**: 构建Pandas DataFrame进行统计分析
- **可视化**: 使用Streamlit的`st.bar_chart()`绘制对比图

### 后端 (无需修改)
- 复用现有的 `POST /api/v1/evaluation/auto` 接口
- 自动保存评估结果到数据库

### 配置 (`frontend/components/sidebar.py`)
- 新增 `st.session_state.eval_config` 存储评估配置
- 与RAG配置、LLM配置并列

---

## 📝 注意事项

1. **评估时间**: 
   - LLM评估: ~2-3秒/RAG
   - Ragas评估: ~8-10秒/RAG
   - 8个RAG全评估: 约1-2分钟

2. **API限流**: 
   - 批量评估已加入0.5秒延迟，避免API过载
   - 如遇超时，可调大 `timeout=120` 参数

3. **数据持久化**:
   - 评估结果存储在 `st.session_state.eval_results`
   - 页面刷新后需要重新评估
   - 后端数据库有永久存储

4. **Ragas局限**:
   - 目前仅支持2个指标
   - 如需更多指标，需要提供测试集（ground_truth）

---

## 🎯 下一步优化建议

1. **测试集支持**: 允许用户上传测试集，启用完整Ragas指标
2. **历史对比**: 支持查看历史评估记录，分析趋势
3. **导出报告**: 导出评估结果为PDF/Excel
4. **自定义权重**: 允许用户调整各指标权重
5. **A/B测试**: 支持对同一问题进行多轮评估，取平均值

---

## 📚 相关文件

### 修改的文件
- `frontend/components/sidebar.py` - 新增评估配置
- `frontend/components/rag_comparison.py` - 重构为批量评估+统计对比
- `backend/core/ragas_evaluator.py` - 移除需要ground_truth的指标

### 依赖的后端接口
- `POST /api/v1/evaluation/auto` - 单个QA记录自动评估
- `GET /api/v1/evaluation/qa/{qa_record_id}` - 获取评估结果（可选）

### 相关文档
- `AUTO_EVALUATION_GUIDE.md` - 自动评估完整指南
- `AUTO_EVALUATION_SUMMARY.md` - 自动评估实现总结
- `README.md` - 项目主文档

---

## ✅ 完成清单

- [x] 侧边栏新增评估配置
- [x] 批量评估功能实现
- [x] 统计对比表格
- [x] 可视化对比图表
- [x] 推荐系统
- [x] 进度条显示
- [x] Ragas指标优化
- [x] 错误处理
- [x] 更新文档

---

## 🎉 总结

本次更新实现了真正的**批量多RAG技术自动评估**和**全方位统计对比**功能，解决了原版"每个RAG单独评估"的低效问题。现在用户可以：

1. **一键评估**所有选择的RAG技术
2. **可视化对比**各维度指标
3. **智能推荐**最佳RAG策略
4. **灵活配置**评估选项

这为RAG技术的实验和选型提供了**强大的量化工具**！ 🚀

