"""
ç»Ÿè®¡åˆ†æé¡µé¢ - Page 3
åŒ…å«ï¼šå¯¹æ¯”è¡¨æ ¼ã€å¯è§†åŒ–å›¾è¡¨ã€æ¨èã€LLMç”Ÿæˆåˆ†ææŠ¥å‘Š
"""
import streamlit as st
import pandas as pd
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

API_BASE_URL = "http://localhost:8000/api/v1"


def render_statistics_page():
    """
    ç»Ÿè®¡åˆ†æé¡µé¢å¸ƒå±€ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ è¯„ä¼°æŒ‰é’®                  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ å¯¹æ¯”è¡¨æ ¼      â”‚ å¯è§†åŒ–    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ æ¨è                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ AI åˆ†ææŠ¥å‘Š               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    if not st.session_state.rag_results:
        st.info("ğŸ’¡ æš‚æ— ç»Ÿè®¡æ•°æ®ï¼Œè¯·å…ˆåœ¨ã€Œä¸»é¡µé¢ã€è¿›è¡Œæé—®")
        return
    
    st.markdown("## ğŸ“ˆ ç»Ÿè®¡åˆ†æä¸å¯¹æ¯”")
    
    # ========== 1. é¡¶éƒ¨ï¼šè¯„ä¼°æŒ‰é’® ==========
    eval_col1, eval_col2, eval_col3 = st.columns([3, 1, 1])
    
    with eval_col1:
        if st.button("ğŸš€ æ‰¹é‡è¯„ä¼°æ‰€æœ‰RAGæŠ€æœ¯", type="primary", use_container_width=True, key="batch_eval"):
            with st.spinner("æ­£åœ¨è¯„ä¼°..."):
                batch_evaluate_all()
    
    with eval_col2:
        eval_config = st.session_state.get("eval_config", {})
        st.caption(f"LLM: âœ…")
    
    with eval_col3:
        ragas_status = "âœ…" if eval_config.get("use_ragas") else "âŒ"
        st.caption(f"Ragas: {ragas_status}")
    
    st.markdown("---")
    
    # ========== 2. ä¸­é—´ï¼šå¯¹æ¯”è¡¨æ ¼ï¼ˆå·¦ï¼‰+ å¯è§†åŒ–ï¼ˆå³ï¼‰==========
    col_table, col_viz = st.columns([5, 5])
    
    with col_table:
        st.markdown("### ğŸ“Š å¯¹æ¯”è¡¨æ ¼")
        render_comparison_table()
    
    with col_viz:
        st.markdown("### ğŸ“‰ å¯è§†åŒ–åˆ†æ")
        render_visualizations()
    
    st.markdown("---")
    
    # ========== 3. æ¨è ==========
    st.markdown("### ğŸ’¡ æ¨è")
    render_recommendations()
    
    st.markdown("---")
    
    # ========== 4. AIåˆ†ææŠ¥å‘Š ==========
    st.markdown("### ğŸ“ AIåˆ†ææŠ¥å‘Š")
    render_ai_report()


def batch_evaluate_all():
    """æ‰¹é‡è¯„ä¼°æ‰€æœ‰RAGæŠ€æœ¯ï¼ˆä¿®å¤ç‰ˆï¼‰"""
    if not st.session_state.rag_results:
        st.warning("æ²¡æœ‰RAGç»“æœå¯ä¾›è¯„ä¼°")
        return
    
    eval_config = st.session_state.get("eval_config", {})
    use_ragas = eval_config.get("use_ragas", False)
    concurrent_num = st.session_state.get("concurrent_num", 3)
    
    # åˆå§‹åŒ–è¯„ä¼°ç»“æœå­˜å‚¨
    if "eval_results" not in st.session_state:
        st.session_state.eval_results = {}
    
    total = len(st.session_state.rag_results)
    completed = 0
    failed = 0
    
    # æ˜¾ç¤ºè¯„ä¼°ä¿¡æ¯
    eval_info = st.empty()
    eval_progress = st.empty()
    
    eval_info.info(f"ğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼° {total} ä¸ªRAGæŠ€æœ¯...")
    
    # å¹¶å‘è¯„ä¼°
    try:
        with ThreadPoolExecutor(max_workers=concurrent_num) as executor:
            future_to_index = {
                executor.submit(
                    evaluate_single_rag,
                    i,
                    result,
                    use_ragas
                ): i
                for i, result in enumerate(st.session_state.rag_results)
            }
            
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    eval_result = future.result()
                    st.session_state.eval_results[index] = eval_result
                    
                    if eval_result.get("evaluation_success"):
                        completed += 1
                    else:
                        failed += 1
                    
                    # æ›´æ–°è¿›åº¦
                    progress = (completed + failed) / total
                    eval_progress.progress(progress, text=f"è¿›åº¦: {completed + failed}/{total}")
                    
                except Exception as e:
                    failed += 1
                    st.session_state.eval_results[index] = {
                        "evaluation_success": False,
                        "error": str(e)
                    }
        
        # æ˜¾ç¤ºå®Œæˆä¿¡æ¯
        eval_info.success(f"ğŸ‰ è¯„ä¼°å®Œæˆï¼æˆåŠŸ: {completed}, å¤±è´¥: {failed}")
        eval_progress.empty()
        
        # è‡ªåŠ¨åˆ·æ–°é¡µé¢æ˜¾ç¤ºç»“æœ
        time.sleep(1)
        st.rerun()
        
    except Exception as e:
        eval_info.error(f"âŒ è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}")
        eval_progress.empty()


def evaluate_single_rag(index: int, result: dict, use_ragas: bool) -> dict:
    """è¯„ä¼°å•ä¸ªRAGæŠ€æœ¯"""
    # qa_record_idæ˜¯resultçš„ç›´æ¥å­—æ®µï¼Œä¸åœ¨metadataä¸­
    qa_record_id = result.get("qa_record_id")
    
    if not qa_record_id:
        return {"evaluation_success": False, "error": f"ç¼ºå°‘qa_record_idï¼Œresult keys: {list(result.keys())}"}
    
    try:
        response = requests.post(
            f"{API_BASE_URL}/evaluation/auto",
            json={
                "qa_record_id": qa_record_id,
                "use_llm": True,
                "use_ragas": use_ragas
            },
            timeout=180
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            return {"evaluation_success": False, "error": response.text}
    
    except Exception as e:
        return {"evaluation_success": False, "error": str(e)}


def render_comparison_table():
    """æ¸²æŸ“å¯¹æ¯”è¡¨æ ¼"""
    if not st.session_state.rag_results:
        st.caption("æš‚æ— æ•°æ®")
        return
    
    # æ„å»ºè¡¨æ ¼æ•°æ®
    table_data = []
    
    for i, result in enumerate(st.session_state.rag_results):
        row = {
            "RAGæŠ€æœ¯": result["rag_technique"],
            "æ‰§è¡Œæ—¶é—´(s)": f"{result['execution_time']:.2f}",
            "ç»¼åˆå¾—åˆ†": "-",
            "ç›¸å…³æ€§": "-",
            "å¿ å®åº¦": "-",
            "è¿è´¯æ€§": "-",
            "Ragaså¿ å®åº¦": "-",
            "Ragasç›¸å…³æ€§": "-"
        }
        
        # æ·»åŠ è¯„ä¼°ç»“æœ
        if "eval_results" in st.session_state and i in st.session_state.eval_results:
            eval_result = st.session_state.eval_results[i]
            if eval_result.get("evaluation_success"):
                llm_eval = eval_result.get("llm_evaluation", {})
                
                # åç«¯è¿”å›çš„å­—æ®µåæ˜¯ relevance_score, faithfulness_score ç­‰
                row["ç»¼åˆå¾—åˆ†"] = f"{llm_eval.get('overall_score', 0):.1f}"
                row["ç›¸å…³æ€§"] = f"{llm_eval.get('relevance_score', 0):.1f}"
                row["å¿ å®åº¦"] = f"{llm_eval.get('faithfulness_score', 0):.1f}"
                row["è¿è´¯æ€§"] = f"{llm_eval.get('coherence_score', 0):.1f}"
                
                if eval_result.get("ragas_evaluation"):
                    ragas_eval = eval_result["ragas_evaluation"]
                    row["Ragaså¿ å®åº¦"] = f"{ragas_eval.get('faithfulness', 0):.3f}"
                    row["Ragasç›¸å…³æ€§"] = f"{ragas_eval.get('answer_relevancy', 0):.3f}"
        
        table_data.append(row)
    
    df = pd.DataFrame(table_data)
    
    # æ˜¾ç¤ºè¡¨æ ¼ï¼ˆå¸¦æ ·å¼ï¼‰
    st.dataframe(
        df,
        use_container_width=True,
        hide_index=True,
        height=400
    )
    
    # ä¸‹è½½æŒ‰é’®
    csv = df.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½CSV",
        data=csv,
        file_name="rag_comparison.csv",
        mime="text/csv"
    )


def render_visualizations():
    """æ¸²æŸ“å¯è§†åŒ–å›¾è¡¨"""
    if not st.session_state.rag_results:
        st.caption("æš‚æ— æ•°æ®")
        return
    
    # å‡†å¤‡æ•°æ®
    techniques = [r["rag_technique"] for r in st.session_state.rag_results]
    exec_times = [r["execution_time"] for r in st.session_state.rag_results]
    
    # æå–è¯„åˆ†æ•°æ®
    overall_scores = []
    relevance_scores = []
    faithfulness_scores = []
    
    for i, result in enumerate(st.session_state.rag_results):
        if "eval_results" in st.session_state and i in st.session_state.eval_results:
            eval_result = st.session_state.eval_results[i]
            if eval_result.get("evaluation_success"):
                llm_eval = eval_result.get("llm_evaluation", {})
                
                # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µåï¼šrelevance_score, faithfulness_score
                overall_scores.append(llm_eval.get("overall_score", 0))
                relevance_scores.append(llm_eval.get("relevance_score", 0))
                faithfulness_scores.append(llm_eval.get("faithfulness_score", 0))
            else:
                overall_scores.append(0)
                relevance_scores.append(0)
                faithfulness_scores.append(0)
        else:
            overall_scores.append(0)
            relevance_scores.append(0)
            faithfulness_scores.append(0)
    
    # ä¸‰ä¸ªå›¾è¡¨
    viz_tab1, viz_tab2, viz_tab3 = st.tabs(["â±ï¸ æ‰§è¡Œæ—¶é—´", "ğŸ“Š LLMè¯„åˆ†", "ğŸ“ˆ æ€§èƒ½å¯¹æ¯”"])
    
    with viz_tab1:
        # æ‰§è¡Œæ—¶é—´æŸ±çŠ¶å›¾
        df_time = pd.DataFrame({
            "RAGæŠ€æœ¯": techniques,
            "æ‰§è¡Œæ—¶é—´(ç§’)": exec_times
        })
        st.bar_chart(df_time.set_index("RAGæŠ€æœ¯"))
    
    with viz_tab2:
        # LLMè¯„åˆ†å¯¹æ¯”
        if any(s > 0 for s in overall_scores):
            df_scores = pd.DataFrame({
                "RAGæŠ€æœ¯": techniques,
                "ç»¼åˆå¾—åˆ†": overall_scores,
                "ç›¸å…³æ€§": relevance_scores,
                "å¿ å®åº¦": faithfulness_scores
            })
            st.line_chart(df_scores.set_index("RAGæŠ€æœ¯"))
        else:
            st.info("è¯·å…ˆè¿›è¡Œæ‰¹é‡è¯„ä¼°")
    
    with viz_tab3:
        # æ€§èƒ½å¯¹æ¯”ï¼ˆæ•£ç‚¹å›¾ï¼šæ‰§è¡Œæ—¶é—´ vs ç»¼åˆå¾—åˆ†ï¼‰
        if any(s > 0 for s in overall_scores):
            df_perf = pd.DataFrame({
                "RAGæŠ€æœ¯": techniques,
                "æ‰§è¡Œæ—¶é—´": exec_times,
                "ç»¼åˆå¾—åˆ†": overall_scores
            })
            st.scatter_chart(df_perf, x="æ‰§è¡Œæ—¶é—´", y="ç»¼åˆå¾—åˆ†", size=20)
        else:
            st.info("è¯·å…ˆè¿›è¡Œæ‰¹é‡è¯„ä¼°")


def render_recommendations():
    """æ¸²æŸ“æ¨è"""
    if "eval_results" not in st.session_state or not st.session_state.eval_results:
        st.info("ğŸ’¡ è¯·å…ˆè¿›è¡Œæ‰¹é‡è¯„ä¼°ä»¥è·å–æ¨è")
        return
    
    # è®¡ç®—Top 3
    rankings = []
    
    for i, result in enumerate(st.session_state.rag_results):
        if i in st.session_state.eval_results:
            eval_result = st.session_state.eval_results[i]
            if eval_result.get("evaluation_success"):
                llm_eval = eval_result.get("llm_evaluation", {})
                overall_score = llm_eval.get("overall_score", 0)
                exec_time = result["execution_time"]
                
                # ç»¼åˆåˆ†æ•°ï¼šè´¨é‡(70%) + é€Ÿåº¦(30%)
                time_score = max(0, 10 - exec_time)  # æ—¶é—´è¶ŠçŸ­å¾—åˆ†è¶Šé«˜
                combined_score = 0.7 * overall_score + 0.3 * time_score
                
                rankings.append({
                    "rag_technique": result["rag_technique"],
                    "overall_score": overall_score,
                    "exec_time": exec_time,
                    "combined_score": combined_score
                })
    
    if not rankings:
        st.warning("æš‚æ— æœ‰æ•ˆè¯„åˆ†")
        return
    
    # æ’åº
    rankings.sort(key=lambda x: x["combined_score"], reverse=True)
    
    # æ˜¾ç¤ºTop 3
    st.markdown("#### ğŸ† Top 3 æ¨èRAGæŠ€æœ¯")
    
    top3 = rankings[:3]
    
    cols = st.columns(3)
    
    medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"]
    
    for i, (col, item) in enumerate(zip(cols, top3)):
        with col:
            st.markdown(f"### {medals[i]} {item['rag_technique']}")
            st.metric("ç»¼åˆè¯„åˆ†", f"{item['combined_score']:.2f}/10")
            st.caption(f"è´¨é‡: {item['overall_score']:.1f} | é€Ÿåº¦: {item['exec_time']:.2f}s")


def render_ai_report():
    """æ¸²æŸ“AIç”Ÿæˆçš„åˆ†ææŠ¥å‘Š"""
    if not st.session_state.rag_results:
        st.caption("æš‚æ— æ•°æ®")
        return
    
    if st.button("âœ¨ ç”ŸæˆAIåˆ†ææŠ¥å‘Š", type="primary"):
        with st.spinner("ğŸ¤– AIæ­£åœ¨åˆ†æ..."):
            report = generate_ai_report()
            st.markdown(report)


def generate_ai_report() -> str:
    """ç”ŸæˆAIåˆ†ææŠ¥å‘Šï¼ˆè°ƒç”¨LLMï¼‰"""
    # æ”¶é›†æ•°æ®
    summary_data = []
    
    for i, result in enumerate(st.session_state.rag_results):
        item = {
            "æŠ€æœ¯": result["rag_technique"],
            "æ—¶é—´": result["execution_time"]
        }
        
        if "eval_results" in st.session_state and i in st.session_state.eval_results:
            eval_result = st.session_state.eval_results[i]
            if eval_result.get("evaluation_success"):
                llm_eval = eval_result.get("llm_evaluation", {})
                item["å¾—åˆ†"] = llm_eval.get("overall_score", 0)
        
        summary_data.append(item)
    
    # æ„å»ºprompt
    prompt = f"""è¯·åˆ†æä»¥ä¸‹RAGæŠ€æœ¯å¯¹æ¯”ç»“æœï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„åˆ†ææŠ¥å‘Šã€‚

æ•°æ®ï¼š
{summary_data}

è¦æ±‚ï¼š
1. æ€»ç»“å„RAGæŠ€æœ¯çš„ä¼˜åŠ£
2. åˆ†ææ€§èƒ½å’Œè´¨é‡çš„æƒè¡¡
3. ç»™å‡ºåº”ç”¨åœºæ™¯å»ºè®®
4. æå‡ºæ”¹è¿›æ–¹å‘

æŠ¥å‘Šæ ¼å¼ï¼šMarkdown
"""
    
    try:
        from backend.utils.llm import call_llm
        
        report = call_llm(
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„RAGæŠ€æœ¯åˆ†æå¸ˆ"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1500
        )
        
        return report
    
    except Exception as e:
        return f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥ï¼š{str(e)}\n\nè¯·ç¡®ä¿åç«¯æœåŠ¡æ­£å¸¸è¿è¡Œã€‚"

