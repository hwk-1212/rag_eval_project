"""
ç»Ÿè®¡åˆ†æé¡µé¢ - Page 3
åŒ…å«ï¼šå¯¹æ¯”è¡¨æ ¼ã€å¯è§†åŒ–å›¾è¡¨ã€æ¨èã€LLMç”Ÿæˆåˆ†ææŠ¥å‘Š
"""
import streamlit as st
import pandas as pd
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from loguru import logger

API_BASE_URL = "http://localhost:8000/api/v1"


def render_statistics_page():
    """
    ç»Ÿè®¡åˆ†æé¡µé¢å¸ƒå±€ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ è¯„ä¼°æŒ‰é’®                  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ å¯¹æ¯”è¡¨æ ¼      â”‚ å¯è§†åŒ–    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ æ¨è                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ AI åˆ†ææŠ¥å‘Š               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    if not st.session_state.rag_results:
        st.info("ğŸ’¡ æš‚æ— ç»Ÿè®¡æ•°æ®ï¼Œè¯·å…ˆåœ¨ã€Œä¸»é¡µé¢ã€è¿›è¡Œæé—®")
        return
    
    st.markdown("## ğŸ“ˆ ç»Ÿè®¡åˆ†æä¸å¯¹æ¯”")
    
    # ========== 1. é¡¶éƒ¨ï¼šè¯„ä¼°æŒ‰é’® ==========
    eval_col1, eval_col2, eval_col3 = st.columns([3, 1, 1])
    
    with eval_col1:
        if st.button("ğŸš€ æ‰¹é‡è¯„ä¼°æ‰€æœ‰RAGæŠ€æœ¯", type="primary", use_container_width=True, key="batch_eval"):
            with st.spinner("æ­£åœ¨è¯„ä¼°..."):
                batch_evaluate_all()
    
    with eval_col2:
        eval_config = st.session_state.get("eval_config", {})
        st.caption(f"LLM: âœ…")
    
    with eval_col3:
        ragas_status = "âœ…" if eval_config.get("use_ragas") else "âŒ"
        st.caption(f"Ragas: {ragas_status}")
    
    st.markdown("---")
    
    # ========== 2. ä¸­é—´ï¼šå¯¹æ¯”è¡¨æ ¼ï¼ˆå·¦ï¼‰+ å¯è§†åŒ–ï¼ˆå³ï¼‰==========
    col_table, col_viz = st.columns([5, 5])
    
    with col_table:
        st.markdown("### ğŸ“Š å¯¹æ¯”è¡¨æ ¼")
        render_comparison_table()
    
    with col_viz:
        st.markdown("### ğŸ“‰ å¯è§†åŒ–åˆ†æ")
        render_visualizations()
    
    st.markdown("---")
    
    # ========== 3. æ¨è ==========
    st.markdown("### ğŸ’¡ æ¨è")
    render_recommendations()
    
    st.markdown("---")
    
    # ========== 4. AIåˆ†ææŠ¥å‘Š ==========
    st.markdown("### ğŸ“ AIåˆ†ææŠ¥å‘Š")
    render_ai_report()


def batch_evaluate_all():
    """æ‰¹é‡è¯„ä¼°æ‰€æœ‰RAGæŠ€æœ¯ï¼ˆä¿®å¤ç‰ˆï¼‰"""
    if not st.session_state.rag_results:
        st.warning("æ²¡æœ‰RAGç»“æœå¯ä¾›è¯„ä¼°")
        return
    
    eval_config = st.session_state.get("eval_config", {})
    use_ragas = eval_config.get("use_ragas", False)
    concurrent_num = st.session_state.get("concurrent_num", 3)
    
    # åˆå§‹åŒ–è¯„ä¼°ç»“æœå­˜å‚¨
    if "eval_results" not in st.session_state:
        st.session_state.eval_results = {}
    
    total = len(st.session_state.rag_results)
    completed = 0
    failed = 0
    
    # æ˜¾ç¤ºè¯„ä¼°ä¿¡æ¯
    eval_info = st.empty()
    eval_progress = st.empty()
    
    eval_info.info(f"ğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼° {total} ä¸ªRAGæŠ€æœ¯...")
    
    # å¹¶å‘è¯„ä¼°
    try:
        with ThreadPoolExecutor(max_workers=concurrent_num) as executor:
            future_to_index = {
                executor.submit(
                    evaluate_single_rag,
                    i,
                    result,
                    use_ragas
                ): i
                for i, result in enumerate(st.session_state.rag_results)
            }
            
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    eval_result = future.result()
                    st.session_state.eval_results[index] = eval_result
                    
                    if eval_result.get("evaluation_success"):
                        completed += 1
                    else:
                        failed += 1
                    
                    # æ›´æ–°è¿›åº¦
                    progress = (completed + failed) / total
                    eval_progress.progress(progress, text=f"è¿›åº¦: {completed + failed}/{total}")
                    
                except Exception as e:
                    failed += 1
                    st.session_state.eval_results[index] = {
                        "evaluation_success": False,
                        "error": str(e)
                    }
        
        # æ˜¾ç¤ºå®Œæˆä¿¡æ¯
        eval_info.success(f"ğŸ‰ è¯„ä¼°å®Œæˆï¼æˆåŠŸ: {completed}, å¤±è´¥: {failed}")
        eval_progress.empty()
        
        # è‡ªåŠ¨åˆ·æ–°é¡µé¢æ˜¾ç¤ºç»“æœ
        time.sleep(1)
        st.rerun()
        
    except Exception as e:
        eval_info.error(f"âŒ è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}")
        eval_progress.empty()


def evaluate_single_rag(index: int, result: dict, use_ragas: bool) -> dict:
    """è¯„ä¼°å•ä¸ªRAGæŠ€æœ¯"""
    # qa_record_idæ˜¯resultçš„ç›´æ¥å­—æ®µï¼Œä¸åœ¨metadataä¸­
    qa_record_id = result.get("qa_record_id")
    
    if not qa_record_id:
        return {"evaluation_success": False, "error": f"ç¼ºå°‘qa_record_idï¼Œresult keys: {list(result.keys())}"}
    
    try:
        response = requests.post(
            f"{API_BASE_URL}/evaluation/auto",
            json={
                "qa_record_id": qa_record_id,
                "use_llm": True,
                "use_ragas": use_ragas
            },
            timeout=180
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            return {"evaluation_success": False, "error": response.text}
    
    except Exception as e:
        return {"evaluation_success": False, "error": str(e)}


def load_evaluations_from_db():
    """ä»æ•°æ®åº“åŠ è½½è¯„ä¼°æ•°æ®"""
    if not st.session_state.rag_results:
        return
    
    try:
        # ä¸ºæ¯ä¸ªRAGç»“æœåŠ è½½æ•°æ®åº“ä¸­çš„è¯„ä¼°æ•°æ®
        for i, result in enumerate(st.session_state.rag_results):
            qa_record_id = result.get("qa_record_id")
            if not qa_record_id:
                continue
            
            # è°ƒç”¨APIè·å–è¯„ä¼°æ•°æ®ï¼ˆä½¿ç”¨å…¨å±€å¸¸é‡ï¼‰
            response = requests.get(
                f"{API_BASE_URL}/evaluations/qa_record/{qa_record_id}",
                timeout=10
            )
            
            if response.status_code == 200:
                evaluations = response.json()
                if evaluations:
                    # è·å–æœ€æ–°çš„autoç±»å‹è¯„ä¼°
                    auto_evals = [e for e in evaluations if e.get("score_type") == "auto"]
                    if auto_evals:
                        latest_eval = auto_evals[-1]  # æœ€æ–°çš„è¯„ä¼°
                        
                        # åˆå§‹åŒ–eval_resultså­—å…¸
                        if "eval_results" not in st.session_state:
                            st.session_state.eval_results = {}
                        
                        # æ„é€ eval_resultæ ¼å¼
                        eval_result = {
                            "evaluation_success": True,
                            "llm_evaluation": {
                                "overall_score": latest_eval.get("overall_score", 0),
                                "relevance_score": latest_eval.get("relevance_score", 0),
                                "faithfulness_score": latest_eval.get("faithfulness_score", 0),
                                "coherence_score": latest_eval.get("coherence_score", 0),
                                "fluency_score": latest_eval.get("fluency_score", 0),
                                "conciseness_score": latest_eval.get("conciseness_score", 0),
                            }
                        }
                        
                        # ä»metadataä¸­æå–Ragasè¯„åˆ†
                        metadata = latest_eval.get("meta_data") or latest_eval.get("metadata", {})
                        if metadata and "ragas_scores" in metadata:
                            ragas_scores = metadata["ragas_scores"]
                            eval_result["ragas_evaluation"] = {
                                "faithfulness": ragas_scores.get("faithfulness", 0),
                                "answer_relevancy": ragas_scores.get("answer_relevancy", 0),
                                "context_precision": ragas_scores.get("context_precision", 0),
                                "context_recall": ragas_scores.get("context_recall", 0),
                            }
                        
                        st.session_state.eval_results[i] = eval_result
    except Exception as e:
        logger.warning(f"ä»æ•°æ®åº“åŠ è½½è¯„ä¼°æ•°æ®å¤±è´¥: {e}")


def render_comparison_table():
    """æ¸²æŸ“å¯¹æ¯”è¡¨æ ¼"""
    if not st.session_state.rag_results:
        st.caption("æš‚æ— æ•°æ®")
        return
    
    # å°è¯•ä»æ•°æ®åº“åŠ è½½è¯„ä¼°æ•°æ®ï¼ˆå¦‚æœsession_stateä¸­æ²¡æœ‰ï¼‰
    if "eval_results" not in st.session_state or not st.session_state.eval_results:
        load_evaluations_from_db()
    
    # æ„å»ºè¡¨æ ¼æ•°æ®
    table_data = []
    
    for i, result in enumerate(st.session_state.rag_results):
        row = {
            "RAGæŠ€æœ¯": result["rag_technique"],
            "æ‰§è¡Œæ—¶é—´(s)": f"{result['execution_time']:.2f}",
            "ç»¼åˆå¾—åˆ†": "-",
            "ç›¸å…³æ€§": "-",
            "å¿ å®åº¦": "-",
            "è¿è´¯æ€§": "-",
            "æµç•…æ€§": "-",
            "ç®€æ´æ€§": "-",
            "Ragaså¿ å®åº¦": "-",
            "Ragasç›¸å…³æ€§": "-"
        }
        
        # æ·»åŠ è¯„ä¼°ç»“æœ
        if "eval_results" in st.session_state and i in st.session_state.eval_results:
            eval_result = st.session_state.eval_results[i]
            if eval_result.get("evaluation_success"):
                llm_eval = eval_result.get("llm_evaluation", {})
                
                # åç«¯è¿”å›çš„å­—æ®µåæ˜¯ relevance_score, faithfulness_score ç­‰
                row["ç»¼åˆå¾—åˆ†"] = f"{llm_eval.get('overall_score', 0):.1f}"
                row["ç›¸å…³æ€§"] = f"{llm_eval.get('relevance_score', 0):.1f}"
                row["å¿ å®åº¦"] = f"{llm_eval.get('faithfulness_score', 0):.1f}"
                row["è¿è´¯æ€§"] = f"{llm_eval.get('coherence_score', 0):.1f}"
                row["æµç•…æ€§"] = f"{llm_eval.get('fluency_score', 0):.1f}"
                row["ç®€æ´æ€§"] = f"{llm_eval.get('conciseness_score', 0):.1f}"
                
                if eval_result.get("ragas_evaluation"):
                    ragas_eval = eval_result["ragas_evaluation"]
                    # åªå±•ç¤ºå®é™…è¯„ä¼°çš„RagasæŒ‡æ ‡ï¼ˆéé›¶å€¼ï¼‰
                    faithfulness = ragas_eval.get('faithfulness', 0)
                    answer_relevancy = ragas_eval.get('answer_relevancy', 0)
                    if faithfulness > 0:
                        row["Ragaså¿ å®åº¦"] = f"{faithfulness:.3f}"
                    if answer_relevancy > 0:
                        row["Ragasç›¸å…³æ€§"] = f"{answer_relevancy:.3f}"
        
        table_data.append(row)
    
    df = pd.DataFrame(table_data)
    
    # æ˜¾ç¤ºè¡¨æ ¼ï¼ˆå¸¦æ ·å¼ï¼‰
    st.dataframe(
        df,
        use_container_width=True,
        hide_index=True,
        height=400
    )
    
    # ä¸‹è½½æŒ‰é’®
    csv = df.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½CSV",
        data=csv,
        file_name="rag_comparison.csv",
        mime="text/csv"
    )


def render_visualizations():
    """æ¸²æŸ“å¯è§†åŒ–å›¾è¡¨"""
    if not st.session_state.rag_results:
        st.caption("æš‚æ— æ•°æ®")
        return
    
    # å°è¯•ä»æ•°æ®åº“åŠ è½½è¯„ä¼°æ•°æ®ï¼ˆå¦‚æœsession_stateä¸­æ²¡æœ‰ï¼‰
    if "eval_results" not in st.session_state or not st.session_state.eval_results:
        load_evaluations_from_db()
    
    # å‡†å¤‡æ•°æ®
    techniques = [r["rag_technique"] for r in st.session_state.rag_results]
    exec_times = [r["execution_time"] for r in st.session_state.rag_results]
    
    # æå–è¯„åˆ†æ•°æ® - æ‰€æœ‰LLMè¯„ä»·ç»´åº¦
    overall_scores = []
    relevance_scores = []
    faithfulness_scores = []
    coherence_scores = []
    fluency_scores = []
    conciseness_scores = []
    
    for i, result in enumerate(st.session_state.rag_results):
        if "eval_results" in st.session_state and i in st.session_state.eval_results:
            eval_result = st.session_state.eval_results[i]
            if eval_result.get("evaluation_success"):
                llm_eval = eval_result.get("llm_evaluation", {})
                
                # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
                overall_scores.append(llm_eval.get("overall_score", 0))
                relevance_scores.append(llm_eval.get("relevance_score", 0))
                faithfulness_scores.append(llm_eval.get("faithfulness_score", 0))
                coherence_scores.append(llm_eval.get("coherence_score", 0))
                fluency_scores.append(llm_eval.get("fluency_score", 0))
                conciseness_scores.append(llm_eval.get("conciseness_score", 0))
            else:
                overall_scores.append(0)
                relevance_scores.append(0)
                faithfulness_scores.append(0)
                coherence_scores.append(0)
                fluency_scores.append(0)
                conciseness_scores.append(0)
        else:
            overall_scores.append(0)
            relevance_scores.append(0)
            faithfulness_scores.append(0)
            coherence_scores.append(0)
            fluency_scores.append(0)
            conciseness_scores.append(0)
    
    # ä¸¤ä¸ªå›¾è¡¨Tab
    viz_tab1, viz_tab2 = st.tabs(["ğŸ“Š LLMè¯„åˆ†å¯¹æ¯”", "ğŸ“ˆ æ€§èƒ½å¯¹æ¯”"])
    
    with viz_tab1:
        # LLMè¯„åˆ†å¯¹æ¯” - å †å æŸ±çŠ¶å›¾
        if any(s > 0 for s in overall_scores):
            try:
                import plotly.graph_objects as go
                
                # åˆ›å»ºå †å æŸ±çŠ¶å›¾
                fig = go.Figure()
                
                # æ·»åŠ æ¯ä¸ªè¯„ä»·ç»´åº¦
                fig.add_trace(go.Bar(
                    name='ç›¸å…³æ€§',
                    x=techniques,
                    y=relevance_scores,
                    marker_color='#FF6B6B'
                ))
                
                fig.add_trace(go.Bar(
                    name='å¿ å®åº¦',
                    x=techniques,
                    y=faithfulness_scores,
                    marker_color='#4ECDC4'
                ))
                
                fig.add_trace(go.Bar(
                    name='è¿è´¯æ€§',
                    x=techniques,
                    y=coherence_scores,
                    marker_color='#45B7D1'
                ))
                
                fig.add_trace(go.Bar(
                    name='æµç•…æ€§',
                    x=techniques,
                    y=fluency_scores,
                    marker_color='#FFA07A'
                ))
                
                fig.add_trace(go.Bar(
                    name='ç®€æ´æ€§',
                    x=techniques,
                    y=conciseness_scores,
                    marker_color='#98D8C8'
                ))
                
                # æ›´æ–°å¸ƒå±€
                fig.update_layout(
                    barmode='stack',
                    title='LLMè¯„åˆ†ç»´åº¦å¯¹æ¯”ï¼ˆå †å æŸ±çŠ¶å›¾ï¼‰',
                    xaxis_title='RAGæŠ€æœ¯',
                    yaxis_title='è¯„åˆ†',
                    height=500,
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=1.02,
                        xanchor="right",
                        x=1
                    )
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
            except ImportError:
                st.error("âš ï¸ Plotlyæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install plotly")
                st.info("æˆ–æ‰§è¡Œè„šæœ¬: bash install_plotly.sh")
                # é™çº§åˆ°ç®€å•å›¾è¡¨
                df_scores = pd.DataFrame({
                    "RAGæŠ€æœ¯": techniques,
                    "ç›¸å…³æ€§": relevance_scores,
                    "å¿ å®åº¦": faithfulness_scores,
                    "è¿è´¯æ€§": coherence_scores,
                    "æµç•…æ€§": fluency_scores,
                    "ç®€æ´æ€§": conciseness_scores
                })
                st.bar_chart(df_scores.set_index("RAGæŠ€æœ¯"))
        else:
            st.info("è¯·å…ˆè¿›è¡Œæ‰¹é‡è¯„ä¼°")
    
    with viz_tab2:
        # æ€§èƒ½å¯¹æ¯” - ç»¼åˆå¾—åˆ†å’Œæ‰§è¡Œæ—¶é—´çš„å¹¶åˆ—æŸ±çŠ¶å›¾
        if any(s > 0 for s in overall_scores):
            try:
                import plotly.graph_objects as go
                from plotly.subplots import make_subplots
                
                # åˆ›å»ºå¹¶åˆ—åŒYè½´å­å›¾
                fig = make_subplots(
                    rows=1, cols=2,
                    subplot_titles=('ç»¼åˆå¾—åˆ†', 'æ‰§è¡Œæ—¶é—´'),
                    specs=[[{"secondary_y": False}, {"secondary_y": False}]]
                )
                
                # å·¦å›¾ï¼šç»¼åˆå¾—åˆ†
                fig.add_trace(
                    go.Bar(
                        x=techniques,
                        y=overall_scores,
                        name='ç»¼åˆå¾—åˆ†',
                        marker_color='#667EEA',
                        text=[f'{s:.1f}' for s in overall_scores],
                        textposition='outside',
                        showlegend=False
                    ),
                    row=1, col=1
                )
                
                # å³å›¾ï¼šæ‰§è¡Œæ—¶é—´
                fig.add_trace(
                    go.Bar(
                        x=techniques,
                        y=exec_times,
                        name='æ‰§è¡Œæ—¶é—´',
                        marker_color='#F093FB',
                        text=[f'{t:.2f}s' for t in exec_times],
                        textposition='outside',
                        showlegend=False
                    ),
                    row=1, col=2
                )
                
                # æ›´æ–°å¸ƒå±€
                fig.update_xaxes(title_text="RAGæŠ€æœ¯", row=1, col=1)
                fig.update_xaxes(title_text="RAGæŠ€æœ¯", row=1, col=2)
                fig.update_yaxes(title_text="å¾—åˆ†ï¼ˆ0-10ï¼‰", range=[0, 10.5], row=1, col=1)
                fig.update_yaxes(title_text="æ—¶é—´ï¼ˆç§’ï¼‰", row=1, col=2)
                
                fig.update_layout(
                    title_text='æ€§èƒ½å¯¹æ¯”ï¼šç»¼åˆå¾—åˆ† vs æ‰§è¡Œæ—¶é—´',
                    height=450,
                    showlegend=False
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
            except ImportError:
                st.error("âš ï¸ Plotlyæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install plotly")
                st.info("æˆ–æ‰§è¡Œè„šæœ¬: bash install_plotly.sh")
                # é™çº§åˆ°ç®€å•å›¾è¡¨
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("##### ç»¼åˆå¾—åˆ†")
                    df_score = pd.DataFrame({
                        "RAGæŠ€æœ¯": techniques,
                        "å¾—åˆ†": overall_scores
                    })
                    st.bar_chart(df_score.set_index("RAGæŠ€æœ¯"))
                
                with col2:
                    st.markdown("##### æ‰§è¡Œæ—¶é—´")
                    df_time = pd.DataFrame({
                        "RAGæŠ€æœ¯": techniques,
                        "æ—¶é—´(ç§’)": exec_times
                    })
                    st.bar_chart(df_time.set_index("RAGæŠ€æœ¯"))
        else:
            st.info("è¯·å…ˆè¿›è¡Œæ‰¹é‡è¯„ä¼°")


def render_recommendations():
    """æ¸²æŸ“æ¨è"""
    # å°è¯•ä»æ•°æ®åº“åŠ è½½è¯„ä¼°æ•°æ®ï¼ˆå¦‚æœsession_stateä¸­æ²¡æœ‰ï¼‰
    if "eval_results" not in st.session_state or not st.session_state.eval_results:
        load_evaluations_from_db()
    
    if "eval_results" not in st.session_state or not st.session_state.eval_results:
        st.info("ğŸ’¡ è¯·å…ˆè¿›è¡Œæ‰¹é‡è¯„ä¼°ä»¥è·å–æ¨è")
        return
    
    # è®¡ç®—Top 3
    rankings = []
    
    for i, result in enumerate(st.session_state.rag_results):
        if i in st.session_state.eval_results:
            eval_result = st.session_state.eval_results[i]
            if eval_result.get("evaluation_success"):
                llm_eval = eval_result.get("llm_evaluation", {})
                overall_score = llm_eval.get("overall_score", 0)
                exec_time = result["execution_time"]
                
                rankings.append({
                    "rag_technique": result["rag_technique"],
                    "overall_score": overall_score,
                    "exec_time": exec_time
                })
    
    if not rankings:
        st.warning("æš‚æ— æœ‰æ•ˆè¯„åˆ†")
        return
    
    # æ’åºï¼ˆæŒ‰overall_scoreæ’åºï¼‰
    rankings.sort(key=lambda x: x["overall_score"], reverse=True)
    
    # æ˜¾ç¤ºTop 3
    st.markdown("#### ğŸ† Top 3 æ¨èRAGæŠ€æœ¯")
    
    top3 = rankings[:3]
    
    cols = st.columns(3)
    
    medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"]
    
    for i, (col, item) in enumerate(zip(cols, top3)):
        with col:
            st.markdown(f"### {medals[i]} {item['rag_technique']}")
            st.metric("ç»¼åˆè¯„åˆ†", f"{item['overall_score']:.1f}/10")
            st.caption(f"æ‰§è¡Œæ—¶é—´: {item['exec_time']:.2f}s")


def render_ai_report():
    """æ¸²æŸ“AIç”Ÿæˆçš„åˆ†ææŠ¥å‘Š"""
    if not st.session_state.rag_results:
        st.caption("æš‚æ— æ•°æ®")
        return
    
    if st.button("âœ¨ ç”ŸæˆAIåˆ†ææŠ¥å‘Š", type="primary"):
        with st.spinner("ğŸ¤– AIæ­£åœ¨åˆ†æ..."):
            report = generate_ai_report()
            st.markdown(report)


def generate_ai_report() -> str:
    """ç”ŸæˆAIåˆ†ææŠ¥å‘Šï¼ˆè°ƒç”¨LLMï¼‰"""
    # æ”¶é›†æ•°æ®
    summary_data = []
    
    for i, result in enumerate(st.session_state.rag_results):
        item = {
            "æŠ€æœ¯": result["rag_technique"],
            "æ—¶é—´": result["execution_time"]
        }
        
        if "eval_results" in st.session_state and i in st.session_state.eval_results:
            eval_result = st.session_state.eval_results[i]
            if eval_result.get("evaluation_success"):
                llm_eval = eval_result.get("llm_evaluation", {})
                item["å¾—åˆ†"] = llm_eval.get("overall_score", 0)
        
        summary_data.append(item)
    
    # æ„å»ºprompt
    prompt = f"""è¯·åˆ†æä»¥ä¸‹RAGæŠ€æœ¯å¯¹æ¯”ç»“æœï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„åˆ†ææŠ¥å‘Šã€‚

æ•°æ®ï¼š
{summary_data}

è¦æ±‚ï¼š
1. æ€»ç»“å„RAGæŠ€æœ¯çš„ä¼˜åŠ£
2. åˆ†ææ€§èƒ½å’Œè´¨é‡çš„æƒè¡¡
3. ç»™å‡ºåº”ç”¨åœºæ™¯å»ºè®®
4. æå‡ºæ”¹è¿›æ–¹å‘

æŠ¥å‘Šæ ¼å¼ï¼šMarkdown
"""
    
    try:
        from backend.utils.llm import call_llm
        
        report = call_llm(
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„RAGæŠ€æœ¯åˆ†æå¸ˆ"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1500
        )
        
        return report
    
    except Exception as e:
        return f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥ï¼š{str(e)}\n\nè¯·ç¡®ä¿åç«¯æœåŠ¡æ­£å¸¸è¿è¡Œã€‚"

