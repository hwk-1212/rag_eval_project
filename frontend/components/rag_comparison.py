import streamlit as st
import requests
import pandas as pd
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

API_BASE_URL = "http://localhost:8000/api/v1"


def render_rag_comparison():
    """æ¸²æŸ“å³ä¾§RAGç»“æœå¯¹æ¯”é¢æ¿"""
    
    st.header("ğŸ“Š RAGç»“æœå¯¹æ¯”")
    
    if not st.session_state.rag_results:
        st.info("æš‚æ— å¯¹æ¯”ç»“æœï¼Œè¯·åœ¨å¯¹è¯çª—å£æé—®")
        return
    
    # åˆ†æˆä¸¤ä¸ªçª—å£ï¼šè‡ªåŠ¨è¯„ä¼° å’Œ ç»Ÿè®¡å¯¹æ¯”
    
    # ========== 1. è‡ªåŠ¨è¯„ä¼°çª—å£ ==========
    st.subheader("ğŸ¤– è‡ªåŠ¨è¯„ä¼°")
    
    # æ‰¹é‡è¯„ä¼°æŒ‰é’®
    eval_config = st.session_state.get("eval_config", {"auto_eval_enabled": False, "use_ragas": False})
    
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        if st.button("ğŸš€ æ‰¹é‡è¯„ä¼°æ‰€æœ‰RAGæŠ€æœ¯", type="primary", use_container_width=True):
            # æ‰§è¡Œæ‰¹é‡è¯„ä¼°
            batch_evaluate_all()
    
    with col2:
        st.caption(f"LLMè¯„ä¼°: âœ…")
    
    with col3:
        ragas_status = "âœ…" if eval_config.get("use_ragas") else "âŒ"
        st.caption(f"Ragas: {ragas_status}")
    
    # æ˜¾ç¤ºå„ä¸ªRAGæŠ€æœ¯çš„ç»“æœ
    tabs = st.tabs([f"{r['rag_technique']}" for r in st.session_state.rag_results])
    
    for i, (tab, result) in enumerate(zip(tabs, st.session_state.rag_results)):
        with tab:
            # ç­”æ¡ˆ
            st.markdown("**ğŸ“ ç­”æ¡ˆ**")
            st.markdown(result["answer"])
            
            # æ‰§è¡Œæ—¶é—´
            col_a, col_b = st.columns(2)
            with col_a:
                st.metric("â±ï¸ æ‰§è¡Œæ—¶é—´", f"{result['execution_time']:.2f}ç§’")
            with col_b:
                # è¯„ä¼°çŠ¶æ€
                if "eval_results" in st.session_state and i in st.session_state.eval_results:
                    eval_result = st.session_state.eval_results[i]
                    if eval_result.get("evaluation_success"):
                        overall_score = eval_result.get("llm_evaluation", {}).get("overall_score", 0)
                        st.metric("ğŸ¯ ç»¼åˆå¾—åˆ†", f"{overall_score:.1f}/10")
                    else:
                        st.caption("âŒ è¯„ä¼°å¤±è´¥")
                else:
                    st.caption("â³ æœªè¯„ä¼°")
            
            # RAGæ‰§è¡Œæ—¥å¿—
            if result.get("metadata", {}).get("execution_logs"):
                with st.expander("ğŸ“‹ æ‰§è¡Œæ—¥å¿—", expanded=False):
                    logs = result["metadata"]["execution_logs"]
                    
                    # æ˜¾ç¤ºæ—¶é—´ç»Ÿè®¡
                    timing = result["metadata"].get("timing", {})
                    if timing:
                        col_time1, col_time2, col_time3 = st.columns(3)
                        with col_time1:
                            st.metric("æ€»è€—æ—¶", f"{timing.get('total', 0):.3f}s")
                        with col_time2:
                            st.metric("æ£€ç´¢", f"{timing.get('retrieve', 0):.3f}s")
                        with col_time3:
                            st.metric("ç”Ÿæˆ", f"{timing.get('generate', 0):.3f}s")
                        st.markdown("---")
                    
                    # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
                    for log in logs:
                        timestamp = log["timestamp"].split("T")[1].split(".")[0]  # åªæ˜¾ç¤ºæ—¶é—´
                        step = log["step"]
                        message = log["message"]
                        details = log.get("details", {})
                        
                        # æ ¹æ®æ­¥éª¤ç±»å‹é€‰æ‹©å›¾æ ‡
                        icon_map = {
                            "init": "ğŸš€",
                            "retrieve_start": "ğŸ”",
                            "retrieve_end": "âœ…",
                            "generate_start": "ğŸ’­",
                            "generate_end": "âœ…",
                            "complete": "ğŸ‰"
                        }
                        icon = icon_map.get(step, "â€¢")
                        
                        st.markdown(f"{icon} `{timestamp}` **{step}**: {message}")
                        
                        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                        if details and step in ["retrieve_end", "generate_end", "complete"]:
                            details_text = " | ".join([f"{k}: {v}" for k, v in details.items()])
                            st.caption(f"    â””â”€ {details_text}")
            
            # æ£€ç´¢æ–‡æ¡£
            with st.expander(f"ğŸ” æ£€ç´¢åˆ°çš„æ–‡æ¡£ ({len(result['retrieved_docs'])}ä¸ª)", expanded=False):
                for j, doc in enumerate(result["retrieved_docs"][:5]):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                    st.text_area(
                        f"æ–‡æ¡£ {j+1} (ç›¸ä¼¼åº¦: {doc['score']:.4f})",
                        doc["content"],
                        height=100,
                        key=f"doc_{i}_{j}",
                        disabled=True
                    )
            
            # è¯„ä¼°è¯¦æƒ…
            if "eval_results" in st.session_state and i in st.session_state.eval_results:
                eval_result = st.session_state.eval_results[i]
                
                if eval_result.get("evaluation_success"):
                    with st.expander("ğŸ“Š è¯¦ç»†è¯„ä¼°ç»“æœ", expanded=False):
                        # LLMè¯„ä¼°
                        if eval_result.get("llm_evaluation"):
                            llm_eval = eval_result["llm_evaluation"]
                            
                            st.markdown("**LLMè¯„åˆ† (0-10)**")
                            cols = st.columns(6)
                            metrics_data = [
                                ("ç›¸å…³æ€§", llm_eval.get('relevance_score', 0)),
                                ("å¿ å®åº¦", llm_eval.get('faithfulness_score', 0)),
                                ("è¿è´¯æ€§", llm_eval.get('coherence_score', 0)),
                                ("æµç•…åº¦", llm_eval.get('fluency_score', 0)),
                                ("ç®€æ´æ€§", llm_eval.get('conciseness_score', 0)),
                                ("ç»¼åˆ", llm_eval.get('overall_score', 0))
                            ]
                            
                            for col, (name, score) in zip(cols, metrics_data):
                                col.metric(name, f"{score:.1f}")
                            
                            if llm_eval.get("feedback"):
                                st.info(f"ğŸ’¡ {llm_eval['feedback']}")
                        
                        # Ragasè¯„ä¼°
                        if eval_result.get("ragas_evaluation") and eval_result["ragas_evaluation"].get("evaluation_success"):
                            ragas_eval = eval_result["ragas_evaluation"]
                            scores = ragas_eval.get("scores", {})
                            
                            st.markdown("**Ragasè¯„ä¼° (0-1)**")
                            cols = st.columns(2)
                            
                            with cols[0]:
                                st.metric("Faithfulness", f"{scores.get('faithfulness', 0):.3f}")
                            with cols[1]:
                                st.metric("Answer Relevancy", f"{scores.get('answer_relevancy', 0):.3f}")
    
    st.markdown("---")
    
    # ========== 2. ç»Ÿè®¡å¯¹æ¯”çª—å£ ==========
    st.subheader("ğŸ“ˆ ç»Ÿè®¡å¯¹æ¯”")
    
    # æ˜¾ç¤ºè¯„ä¼°é…ç½®çŠ¶æ€
    eval_config = st.session_state.get("eval_config", {"auto_eval_enabled": False, "use_ragas": False})
    col_status1, col_status2 = st.columns(2)
    with col_status1:
        llm_status = "âœ… å·²å¯ç”¨" if True else "âŒ æœªå¯ç”¨"
        st.caption(f"LLMè¯„ä¼°: {llm_status}")
    with col_status2:
        ragas_status = "âœ… å·²å¯ç”¨" if eval_config.get("use_ragas", False) else "âŒ æœªå¯ç”¨"
        st.caption(f"Ragasè¯„ä¼°: {ragas_status}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¯„ä¼°ç»“æœ
    if "eval_results" not in st.session_state or not st.session_state.eval_results:
        st.info("ğŸ‘† ç‚¹å‡»ä¸Šæ–¹ã€Œæ‰¹é‡è¯„ä¼°æ‰€æœ‰RAGæŠ€æœ¯ã€æŒ‰é’®æŸ¥çœ‹ç»Ÿè®¡å¯¹æ¯”")
        st.warning("ğŸ’¡ æç¤ºï¼šå¦‚éœ€Ragasè¯„ä¼°ï¼Œè¯·åœ¨å·¦ä¾§ã€Œè‡ªåŠ¨è¯„ä¼°é…ç½®ã€ä¸­å‹¾é€‰ã€Œä½¿ç”¨Ragasè¯„ä¼°ã€ï¼Œç„¶åé‡æ–°æ‰¹é‡è¯„ä¼°")
        return
    
    # æ„å»ºå¯¹æ¯”æ•°æ®
    comparison_data = []
    
    for i, result in enumerate(st.session_state.rag_results):
        if i in st.session_state.eval_results:
            eval_result = st.session_state.eval_results[i]
            
            if eval_result.get("evaluation_success"):
                row_data = {
                    "RAGæŠ€æœ¯": result["rag_technique"],
                    "æ‰§è¡Œæ—¶é—´(ç§’)": result["execution_time"]
                }
                
                # LLMè¯„åˆ†
                if eval_result.get("llm_evaluation"):
                    llm_eval = eval_result["llm_evaluation"]
                    row_data.update({
                        "ç›¸å…³æ€§": llm_eval.get('relevance_score', 0),
                        "å¿ å®åº¦": llm_eval.get('faithfulness_score', 0),
                        "è¿è´¯æ€§": llm_eval.get('coherence_score', 0),
                        "æµç•…åº¦": llm_eval.get('fluency_score', 0),
                        "ç®€æ´æ€§": llm_eval.get('conciseness_score', 0),
                        "ç»¼åˆå¾—åˆ†": llm_eval.get('overall_score', 0)
                    })
                
                # Ragasè¯„åˆ†
                if eval_result.get("ragas_evaluation") and eval_result["ragas_evaluation"].get("evaluation_success"):
                    ragas_eval = eval_result["ragas_evaluation"]
                    scores = ragas_eval.get("scores", {})
                    row_data.update({
                        "Ragas-Faithfulness": scores.get('faithfulness', 0),
                        "Ragas-Answer_Rel": scores.get('answer_relevancy', 0),
                    })
                
                # æ£€ç´¢è´¨é‡
                final_scores = eval_result.get("final_scores", {})
                if final_scores:
                    row_data.update({
                        "æ£€ç´¢ç²¾ç¡®åº¦": final_scores.get('retrieval_precision', 0),
                        "ä¸Šä¸‹æ–‡ç›¸å…³æ€§": final_scores.get('context_relevance', 0)
                    })
                
                comparison_data.append(row_data)
    
    if comparison_data:
        df = pd.DataFrame(comparison_data)
        
        # æ˜¾ç¤ºè¡¨æ ¼
        st.markdown("#### ğŸ“Š è¯¦ç»†å¯¹æ¯”è¡¨")
        st.dataframe(
            df,
            use_container_width=True,
            hide_index=True,
            column_config={
                "RAGæŠ€æœ¯": st.column_config.TextColumn("RAGæŠ€æœ¯", width="medium"),
                "æ‰§è¡Œæ—¶é—´(ç§’)": st.column_config.NumberColumn("æ‰§è¡Œæ—¶é—´(ç§’)", format="%.2f"),
                "ç›¸å…³æ€§": st.column_config.NumberColumn("ç›¸å…³æ€§", format="%.1f"),
                "å¿ å®åº¦": st.column_config.NumberColumn("å¿ å®åº¦", format="%.1f"),
                "è¿è´¯æ€§": st.column_config.NumberColumn("è¿è´¯æ€§", format="%.1f"),
                "æµç•…åº¦": st.column_config.NumberColumn("æµç•…åº¦", format="%.1f"),
                "ç®€æ´æ€§": st.column_config.NumberColumn("ç®€æ´æ€§", format="%.1f"),
                "ç»¼åˆå¾—åˆ†": st.column_config.NumberColumn("ç»¼åˆå¾—åˆ†", format="%.1f"),
                "Ragas-Faithfulness": st.column_config.NumberColumn("Ragas-Faithfulness", format="%.3f"),
                "Ragas-Answer_Rel": st.column_config.NumberColumn("Ragas-Answer_Rel", format="%.3f"),
                "æ£€ç´¢ç²¾ç¡®åº¦": st.column_config.NumberColumn("æ£€ç´¢ç²¾ç¡®åº¦", format="%.2f"),
                "ä¸Šä¸‹æ–‡ç›¸å…³æ€§": st.column_config.NumberColumn("ä¸Šä¸‹æ–‡ç›¸å…³æ€§", format="%.1f"),
            }
        )
        
        # å¯è§†åŒ–å¯¹æ¯”
        st.markdown("#### ğŸ“Š å¯è§†åŒ–å¯¹æ¯”")
        
        tab1, tab2, tab3 = st.tabs(["LLMè¯„åˆ†å¯¹æ¯”", "Ragaså¯¹æ¯”", "æ€§èƒ½å¯¹æ¯”"])
        
        with tab1:
            # LLMè¯„åˆ†å¯¹æ¯”
            llm_metrics = ["ç›¸å…³æ€§", "å¿ å®åº¦", "è¿è´¯æ€§", "æµç•…åº¦", "ç®€æ´æ€§", "ç»¼åˆå¾—åˆ†"]
            available_metrics = [m for m in llm_metrics if m in df.columns]
            
            if available_metrics:
                chart_data = df.set_index("RAGæŠ€æœ¯")[available_metrics]
                st.bar_chart(chart_data, use_container_width=True)
                
                # æ˜¾ç¤ºæœ€ä¼˜æŠ€æœ¯
                best_tech = df.loc[df["ç»¼åˆå¾—åˆ†"].idxmax(), "RAGæŠ€æœ¯"]
                best_score = df["ç»¼åˆå¾—åˆ†"].max()
                st.success(f"ğŸ† ç»¼åˆå¾—åˆ†æœ€é«˜: **{best_tech}** ({best_score:.1f}/10)")
        
        with tab2:
            # Ragaså¯¹æ¯”
            ragas_metrics = ["Ragas-Faithfulness", "Ragas-Answer_Rel"]
            available_ragas = [m for m in ragas_metrics if m in df.columns]
            
            if available_ragas:
                chart_data = df.set_index("RAGæŠ€æœ¯")[available_ragas]
                st.bar_chart(chart_data, use_container_width=True)
                
                # æ˜¾ç¤ºRagasæœ€ä¼˜
                if "Ragas-Faithfulness" in df.columns:
                    best_faith_tech = df.loc[df["Ragas-Faithfulness"].idxmax(), "RAGæŠ€æœ¯"]
                    best_faith = df["Ragas-Faithfulness"].max()
                    st.info(f"ğŸ¯ Faithfulnessæœ€é«˜: **{best_faith_tech}** ({best_faith:.3f})")
                
                if "Ragas-Answer_Rel" in df.columns:
                    best_rel_tech = df.loc[df["Ragas-Answer_Rel"].idxmax(), "RAGæŠ€æœ¯"]
                    best_rel = df["Ragas-Answer_Rel"].max()
                    st.info(f"ğŸ¯ Answer Relevancyæœ€é«˜: **{best_rel_tech}** ({best_rel:.3f})")
            else:
                st.warning("ğŸ“Š æœªå¯ç”¨Ragasè¯„ä¼°")
                st.info("""
                    ğŸ’¡ å¦‚éœ€å¯ç”¨Ragasæ ‡å‡†åŒ–è¯„ä¼°ï¼š
                    
                    1. å‰å¾€å·¦ä¾§æ ã€ŒğŸ¤– è‡ªåŠ¨è¯„ä¼°é…ç½®ã€
                    2. â˜‘ï¸ å‹¾é€‰ã€Œä½¿ç”¨Ragasè¯„ä¼°ã€
                    3. è¿”å›æ­¤é¡µé¢ï¼Œç‚¹å‡»ã€ŒğŸš€ æ‰¹é‡è¯„ä¼°æ‰€æœ‰RAGæŠ€æœ¯ã€
                    
                    â±ï¸ æ³¨æ„ï¼šRagasè¯„ä¼°ä¼šå¢åŠ è¯„ä¼°æ—¶é—´ï¼ˆ~5-8ç§’/RAGï¼‰
                """)
        
        with tab3:
            # æ€§èƒ½å¯¹æ¯”
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**æ‰§è¡Œæ—¶é—´å¯¹æ¯”**")
                time_chart = df.set_index("RAGæŠ€æœ¯")["æ‰§è¡Œæ—¶é—´(ç§’)"]
                st.bar_chart(time_chart, use_container_width=True)
                
                fastest = df.loc[df["æ‰§è¡Œæ—¶é—´(ç§’)"].idxmin(), "RAGæŠ€æœ¯"]
                fastest_time = df["æ‰§è¡Œæ—¶é—´(ç§’)"].min()
                st.caption(f"âš¡ æœ€å¿«: {fastest} ({fastest_time:.2f}ç§’)")
            
            with col2:
                if "æ£€ç´¢ç²¾ç¡®åº¦" in df.columns:
                    st.markdown("**æ£€ç´¢è´¨é‡å¯¹æ¯”**")
                    retrieval_chart = df.set_index("RAGæŠ€æœ¯")["æ£€ç´¢ç²¾ç¡®åº¦"]
                    st.bar_chart(retrieval_chart, use_container_width=True)
                    
                    best_retrieval = df.loc[df["æ£€ç´¢ç²¾ç¡®åº¦"].idxmax(), "RAGæŠ€æœ¯"]
                    best_ret_score = df["æ£€ç´¢ç²¾ç¡®åº¦"].max()
                    st.caption(f"ğŸ¯ æœ€ä¼˜: {best_retrieval} ({best_ret_score:.2f})")
        
        # æ¨è
        st.markdown("#### ğŸ’¡ æ¨è")
        
        # ç»¼åˆè¯„åˆ†å‰3
        top3 = df.nlargest(3, "ç»¼åˆå¾—åˆ†")[["RAGæŠ€æœ¯", "ç»¼åˆå¾—åˆ†", "æ‰§è¡Œæ—¶é—´(ç§’)"]]
        st.markdown("**ç»¼åˆè¯„åˆ†Top 3:**")
        for idx, row in top3.iterrows():
            st.write(f"ğŸ¥‡ **{row['RAGæŠ€æœ¯']}** - å¾—åˆ†: {row['ç»¼åˆå¾—åˆ†']:.1f}/10, è€—æ—¶: {row['æ‰§è¡Œæ—¶é—´(ç§’)']:.2f}ç§’")
    
    else:
        st.warning("è¯„ä¼°æ•°æ®ä¸å®Œæ•´ï¼Œè¯·é‡æ–°è¯„ä¼°")


def batch_evaluate_all():
    """æ‰¹é‡è¯„ä¼°æ‰€æœ‰RAGæŠ€æœ¯ï¼ˆå¹¶å‘ç‰ˆæœ¬ï¼‰"""
    
    eval_config = st.session_state.get("eval_config", {"auto_eval_enabled": True, "use_ragas": False})
    use_ragas = eval_config.get("use_ragas", False)
    concurrent_num = st.session_state.get("concurrent_num", 3)  # è·å–å¹¶å‘æ•°
    
    # æ”¶é›†æ‰€æœ‰qa_record_id
    evaluation_tasks = []
    for i, result in enumerate(st.session_state.rag_results):
        if result.get("qa_record_id"):
            evaluation_tasks.append({
                "index": i,
                "qa_id": result["qa_record_id"],
                "technique": result["rag_technique"]
            })
    
    if not evaluation_tasks:
        st.error("æ— æ³•è·å–QAè®°å½•IDï¼Œè¯·é‡æ–°æŸ¥è¯¢")
        return
    
    # æ˜¾ç¤ºè¿›åº¦
    progress_bar = st.progress(0)
    status_text = st.empty()
    status_text.text(f"ğŸš€ å¼€å§‹å¹¶å‘è¯„ä¼° {len(evaluation_tasks)} ä¸ªRAGæŠ€æœ¯ (å¹¶å‘æ•°: {concurrent_num})")
    
    try:
        # åˆå§‹åŒ–è¯„ä¼°ç»“æœ
        if "eval_results" not in st.session_state:
            st.session_state.eval_results = {}
        
        completed_count = 0
        success_count = 0
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œè¯„ä¼°
        with ThreadPoolExecutor(max_workers=concurrent_num) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(
                    evaluate_single_rag,
                    task["qa_id"],
                    use_ragas
                ): task
                for task in evaluation_tasks
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                completed_count += 1
                
                try:
                    eval_result = future.result()
                    
                    if eval_result and eval_result.get("evaluation_success"):
                        st.session_state.eval_results[task["index"]] = eval_result
                        success_count += 1
                        status_text.text(f"âœ… [{completed_count}/{len(evaluation_tasks)}] {task['technique']} è¯„ä¼°å®Œæˆ")
                    else:
                        status_text.text(f"âš ï¸ [{completed_count}/{len(evaluation_tasks)}] {task['technique']} è¯„ä¼°å¤±è´¥")
                        
                except Exception as e:
                    status_text.text(f"âŒ [{completed_count}/{len(evaluation_tasks)}] {task['technique']} è¯„ä¼°å‡ºé”™: {str(e)}")
                
                # æ›´æ–°è¿›åº¦
                progress_bar.progress(completed_count / len(evaluation_tasks))
        
        # å®Œæˆæç¤º
        status_text.text(f"âœ… è¯„ä¼°å®Œæˆ! æˆåŠŸ: {success_count}/{len(evaluation_tasks)}")
        time.sleep(1.5)
        status_text.empty()
        progress_bar.empty()
        
        st.success(f"âœ… æˆåŠŸè¯„ä¼° {success_count}/{len(evaluation_tasks)} ä¸ªRAGæŠ€æœ¯ (å¹¶å‘æ•°: {concurrent_num})")
        st.rerun()
        
    except Exception as e:
        st.error(f"æ‰¹é‡è¯„ä¼°å¤±è´¥: {str(e)}")
        progress_bar.empty()
        status_text.empty()


def evaluate_single_rag(qa_id: int, use_ragas: bool, timeout: int = 180):
    """
    è¯„ä¼°å•ä¸ªRAGæŠ€æœ¯ï¼ˆç”¨äºå¹¶å‘ï¼‰
    
    Args:
        qa_id: QAè®°å½•ID
        use_ragas: æ˜¯å¦ä½¿ç”¨Ragasè¯„ä¼°
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    try:
        response = requests.post(
            f"{API_BASE_URL}/evaluation/auto",
            json={
                "qa_record_id": qa_id,
                "use_llm_evaluator": True,
                "use_ragas": use_ragas,
                "reference_answer": None
            },
            timeout=timeout  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°3åˆ†é’Ÿ
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            return {"evaluation_success": False, "error": f"HTTP {response.status_code}"}
            
    except requests.exceptions.Timeout:
        return {"evaluation_success": False, "error": "è¯·æ±‚è¶…æ—¶"}
    except Exception as e:
        return {"evaluation_success": False, "error": str(e)}
