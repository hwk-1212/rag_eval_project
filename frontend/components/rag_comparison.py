import streamlit as st
import requests
import pandas as pd
import time

API_BASE_URL = "http://localhost:8000/api/v1"


def render_rag_comparison():
    """æ¸²æŸ“å³ä¾§RAGç»“æœå¯¹æ¯”é¢æ¿"""
    
    st.header("ğŸ“Š RAGç»“æœå¯¹æ¯”")
    
    if not st.session_state.rag_results:
        st.info("æš‚æ— å¯¹æ¯”ç»“æœï¼Œè¯·åœ¨å¯¹è¯çª—å£æé—®")
        return
    
    # åˆ†æˆä¸¤ä¸ªçª—å£ï¼šè‡ªåŠ¨è¯„ä¼° å’Œ ç»Ÿè®¡å¯¹æ¯”
    
    # ========== 1. è‡ªåŠ¨è¯„ä¼°çª—å£ ==========
    st.subheader("ğŸ¤– è‡ªåŠ¨è¯„ä¼°")
    
    # æ‰¹é‡è¯„ä¼°æŒ‰é’®
    eval_config = st.session_state.get("eval_config", {"auto_eval_enabled": False, "use_ragas": False})
    
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        if st.button("ğŸš€ æ‰¹é‡è¯„ä¼°æ‰€æœ‰RAGæŠ€æœ¯", type="primary", use_container_width=True):
            # æ‰§è¡Œæ‰¹é‡è¯„ä¼°
            batch_evaluate_all()
    
    with col2:
        st.caption(f"LLMè¯„ä¼°: âœ…")
    
    with col3:
        ragas_status = "âœ…" if eval_config.get("use_ragas") else "âŒ"
        st.caption(f"Ragas: {ragas_status}")
    
    # æ˜¾ç¤ºå„ä¸ªRAGæŠ€æœ¯çš„ç»“æœ
    tabs = st.tabs([f"{r['rag_technique']}" for r in st.session_state.rag_results])
    
    for i, (tab, result) in enumerate(zip(tabs, st.session_state.rag_results)):
        with tab:
            # ç­”æ¡ˆ
            st.markdown("**ğŸ“ ç­”æ¡ˆ**")
            st.markdown(result["answer"])
            
            # æ‰§è¡Œæ—¶é—´
            col_a, col_b = st.columns(2)
            with col_a:
                st.metric("â±ï¸ æ‰§è¡Œæ—¶é—´", f"{result['execution_time']:.2f}ç§’")
            with col_b:
                # è¯„ä¼°çŠ¶æ€
                if "eval_results" in st.session_state and i in st.session_state.eval_results:
                    eval_result = st.session_state.eval_results[i]
                    if eval_result.get("evaluation_success"):
                        overall_score = eval_result.get("llm_evaluation", {}).get("overall_score", 0)
                        st.metric("ğŸ¯ ç»¼åˆå¾—åˆ†", f"{overall_score:.1f}/10")
                    else:
                        st.caption("âŒ è¯„ä¼°å¤±è´¥")
                else:
                    st.caption("â³ æœªè¯„ä¼°")
            
            # æ£€ç´¢æ–‡æ¡£
            with st.expander(f"ğŸ” æ£€ç´¢åˆ°çš„æ–‡æ¡£ ({len(result['retrieved_docs'])}ä¸ª)", expanded=False):
                for j, doc in enumerate(result["retrieved_docs"][:5]):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                    st.text_area(
                        f"æ–‡æ¡£ {j+1} (ç›¸ä¼¼åº¦: {doc['score']:.4f})",
                        doc["content"],
                        height=100,
                        key=f"doc_{i}_{j}",
                        disabled=True
                    )
            
            # è¯„ä¼°è¯¦æƒ…
            if "eval_results" in st.session_state and i in st.session_state.eval_results:
                eval_result = st.session_state.eval_results[i]
                
                if eval_result.get("evaluation_success"):
                    with st.expander("ğŸ“Š è¯¦ç»†è¯„ä¼°ç»“æœ", expanded=False):
                        # LLMè¯„ä¼°
                        if eval_result.get("llm_evaluation"):
                            llm_eval = eval_result["llm_evaluation"]
                            
                            st.markdown("**LLMè¯„åˆ† (0-10)**")
                            cols = st.columns(6)
                            metrics_data = [
                                ("ç›¸å…³æ€§", llm_eval.get('relevance_score', 0)),
                                ("å¿ å®åº¦", llm_eval.get('faithfulness_score', 0)),
                                ("è¿è´¯æ€§", llm_eval.get('coherence_score', 0)),
                                ("æµç•…åº¦", llm_eval.get('fluency_score', 0)),
                                ("ç®€æ´æ€§", llm_eval.get('conciseness_score', 0)),
                                ("ç»¼åˆ", llm_eval.get('overall_score', 0))
                            ]
                            
                            for col, (name, score) in zip(cols, metrics_data):
                                col.metric(name, f"{score:.1f}")
                            
                            if llm_eval.get("feedback"):
                                st.info(f"ğŸ’¡ {llm_eval['feedback']}")
                        
                        # Ragasè¯„ä¼°
                        if eval_result.get("ragas_evaluation") and eval_result["ragas_evaluation"].get("evaluation_success"):
                            ragas_eval = eval_result["ragas_evaluation"]
                            scores = ragas_eval.get("scores", {})
                            
                            st.markdown("**Ragasè¯„ä¼° (0-1)**")
                            cols = st.columns(2)
                            
                            with cols[0]:
                                st.metric("Faithfulness", f"{scores.get('faithfulness', 0):.3f}")
                            with cols[1]:
                                st.metric("Answer Relevancy", f"{scores.get('answer_relevancy', 0):.3f}")
    
    st.markdown("---")
    
    # ========== 2. ç»Ÿè®¡å¯¹æ¯”çª—å£ ==========
    st.subheader("ğŸ“ˆ ç»Ÿè®¡å¯¹æ¯”")
    
    # æ˜¾ç¤ºè¯„ä¼°é…ç½®çŠ¶æ€
    eval_config = st.session_state.get("eval_config", {"auto_eval_enabled": False, "use_ragas": False})
    col_status1, col_status2 = st.columns(2)
    with col_status1:
        llm_status = "âœ… å·²å¯ç”¨" if True else "âŒ æœªå¯ç”¨"
        st.caption(f"LLMè¯„ä¼°: {llm_status}")
    with col_status2:
        ragas_status = "âœ… å·²å¯ç”¨" if eval_config.get("use_ragas", False) else "âŒ æœªå¯ç”¨"
        st.caption(f"Ragasè¯„ä¼°: {ragas_status}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¯„ä¼°ç»“æœ
    if "eval_results" not in st.session_state or not st.session_state.eval_results:
        st.info("ğŸ‘† ç‚¹å‡»ä¸Šæ–¹ã€Œæ‰¹é‡è¯„ä¼°æ‰€æœ‰RAGæŠ€æœ¯ã€æŒ‰é’®æŸ¥çœ‹ç»Ÿè®¡å¯¹æ¯”")
        st.warning("ğŸ’¡ æç¤ºï¼šå¦‚éœ€Ragasè¯„ä¼°ï¼Œè¯·åœ¨å·¦ä¾§ã€Œè‡ªåŠ¨è¯„ä¼°é…ç½®ã€ä¸­å‹¾é€‰ã€Œä½¿ç”¨Ragasè¯„ä¼°ã€ï¼Œç„¶åé‡æ–°æ‰¹é‡è¯„ä¼°")
        return
    
    # æ„å»ºå¯¹æ¯”æ•°æ®
    comparison_data = []
    
    for i, result in enumerate(st.session_state.rag_results):
        if i in st.session_state.eval_results:
            eval_result = st.session_state.eval_results[i]
            
            if eval_result.get("evaluation_success"):
                row_data = {
                    "RAGæŠ€æœ¯": result["rag_technique"],
                    "æ‰§è¡Œæ—¶é—´(ç§’)": result["execution_time"]
                }
                
                # LLMè¯„åˆ†
                if eval_result.get("llm_evaluation"):
                    llm_eval = eval_result["llm_evaluation"]
                    row_data.update({
                        "ç›¸å…³æ€§": llm_eval.get('relevance_score', 0),
                        "å¿ å®åº¦": llm_eval.get('faithfulness_score', 0),
                        "è¿è´¯æ€§": llm_eval.get('coherence_score', 0),
                        "æµç•…åº¦": llm_eval.get('fluency_score', 0),
                        "ç®€æ´æ€§": llm_eval.get('conciseness_score', 0),
                        "ç»¼åˆå¾—åˆ†": llm_eval.get('overall_score', 0)
                    })
                
                # Ragasè¯„åˆ†
                if eval_result.get("ragas_evaluation") and eval_result["ragas_evaluation"].get("evaluation_success"):
                    ragas_eval = eval_result["ragas_evaluation"]
                    scores = ragas_eval.get("scores", {})
                    row_data.update({
                        "Ragas-Faithfulness": scores.get('faithfulness', 0),
                        "Ragas-Answer_Rel": scores.get('answer_relevancy', 0),
                    })
                
                # æ£€ç´¢è´¨é‡
                final_scores = eval_result.get("final_scores", {})
                if final_scores:
                    row_data.update({
                        "æ£€ç´¢ç²¾ç¡®åº¦": final_scores.get('retrieval_precision', 0),
                        "ä¸Šä¸‹æ–‡ç›¸å…³æ€§": final_scores.get('context_relevance', 0)
                    })
                
                comparison_data.append(row_data)
    
    if comparison_data:
        df = pd.DataFrame(comparison_data)
        
        # æ˜¾ç¤ºè¡¨æ ¼
        st.markdown("#### ğŸ“Š è¯¦ç»†å¯¹æ¯”è¡¨")
        st.dataframe(
            df,
            use_container_width=True,
            hide_index=True,
            column_config={
                "RAGæŠ€æœ¯": st.column_config.TextColumn("RAGæŠ€æœ¯", width="medium"),
                "æ‰§è¡Œæ—¶é—´(ç§’)": st.column_config.NumberColumn("æ‰§è¡Œæ—¶é—´(ç§’)", format="%.2f"),
                "ç›¸å…³æ€§": st.column_config.NumberColumn("ç›¸å…³æ€§", format="%.1f"),
                "å¿ å®åº¦": st.column_config.NumberColumn("å¿ å®åº¦", format="%.1f"),
                "è¿è´¯æ€§": st.column_config.NumberColumn("è¿è´¯æ€§", format="%.1f"),
                "æµç•…åº¦": st.column_config.NumberColumn("æµç•…åº¦", format="%.1f"),
                "ç®€æ´æ€§": st.column_config.NumberColumn("ç®€æ´æ€§", format="%.1f"),
                "ç»¼åˆå¾—åˆ†": st.column_config.NumberColumn("ç»¼åˆå¾—åˆ†", format="%.1f"),
                "Ragas-Faithfulness": st.column_config.NumberColumn("Ragas-Faithfulness", format="%.3f"),
                "Ragas-Answer_Rel": st.column_config.NumberColumn("Ragas-Answer_Rel", format="%.3f"),
                "æ£€ç´¢ç²¾ç¡®åº¦": st.column_config.NumberColumn("æ£€ç´¢ç²¾ç¡®åº¦", format="%.2f"),
                "ä¸Šä¸‹æ–‡ç›¸å…³æ€§": st.column_config.NumberColumn("ä¸Šä¸‹æ–‡ç›¸å…³æ€§", format="%.1f"),
            }
        )
        
        # å¯è§†åŒ–å¯¹æ¯”
        st.markdown("#### ğŸ“Š å¯è§†åŒ–å¯¹æ¯”")
        
        tab1, tab2, tab3 = st.tabs(["LLMè¯„åˆ†å¯¹æ¯”", "Ragaså¯¹æ¯”", "æ€§èƒ½å¯¹æ¯”"])
        
        with tab1:
            # LLMè¯„åˆ†å¯¹æ¯”
            llm_metrics = ["ç›¸å…³æ€§", "å¿ å®åº¦", "è¿è´¯æ€§", "æµç•…åº¦", "ç®€æ´æ€§", "ç»¼åˆå¾—åˆ†"]
            available_metrics = [m for m in llm_metrics if m in df.columns]
            
            if available_metrics:
                chart_data = df.set_index("RAGæŠ€æœ¯")[available_metrics]
                st.bar_chart(chart_data, use_container_width=True)
                
                # æ˜¾ç¤ºæœ€ä¼˜æŠ€æœ¯
                best_tech = df.loc[df["ç»¼åˆå¾—åˆ†"].idxmax(), "RAGæŠ€æœ¯"]
                best_score = df["ç»¼åˆå¾—åˆ†"].max()
                st.success(f"ğŸ† ç»¼åˆå¾—åˆ†æœ€é«˜: **{best_tech}** ({best_score:.1f}/10)")
        
        with tab2:
            # Ragaså¯¹æ¯”
            ragas_metrics = ["Ragas-Faithfulness", "Ragas-Answer_Rel"]
            available_ragas = [m for m in ragas_metrics if m in df.columns]
            
            if available_ragas:
                chart_data = df.set_index("RAGæŠ€æœ¯")[available_ragas]
                st.bar_chart(chart_data, use_container_width=True)
                
                # æ˜¾ç¤ºRagasæœ€ä¼˜
                if "Ragas-Faithfulness" in df.columns:
                    best_faith_tech = df.loc[df["Ragas-Faithfulness"].idxmax(), "RAGæŠ€æœ¯"]
                    best_faith = df["Ragas-Faithfulness"].max()
                    st.info(f"ğŸ¯ Faithfulnessæœ€é«˜: **{best_faith_tech}** ({best_faith:.3f})")
                
                if "Ragas-Answer_Rel" in df.columns:
                    best_rel_tech = df.loc[df["Ragas-Answer_Rel"].idxmax(), "RAGæŠ€æœ¯"]
                    best_rel = df["Ragas-Answer_Rel"].max()
                    st.info(f"ğŸ¯ Answer Relevancyæœ€é«˜: **{best_rel_tech}** ({best_rel:.3f})")
            else:
                st.warning("ğŸ“Š æœªå¯ç”¨Ragasè¯„ä¼°")
                st.info("""
                    ğŸ’¡ å¦‚éœ€å¯ç”¨Ragasæ ‡å‡†åŒ–è¯„ä¼°ï¼š
                    
                    1. å‰å¾€å·¦ä¾§æ ã€ŒğŸ¤– è‡ªåŠ¨è¯„ä¼°é…ç½®ã€
                    2. â˜‘ï¸ å‹¾é€‰ã€Œä½¿ç”¨Ragasè¯„ä¼°ã€
                    3. è¿”å›æ­¤é¡µé¢ï¼Œç‚¹å‡»ã€ŒğŸš€ æ‰¹é‡è¯„ä¼°æ‰€æœ‰RAGæŠ€æœ¯ã€
                    
                    â±ï¸ æ³¨æ„ï¼šRagasè¯„ä¼°ä¼šå¢åŠ è¯„ä¼°æ—¶é—´ï¼ˆ~5-8ç§’/RAGï¼‰
                """)
        
        with tab3:
            # æ€§èƒ½å¯¹æ¯”
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**æ‰§è¡Œæ—¶é—´å¯¹æ¯”**")
                time_chart = df.set_index("RAGæŠ€æœ¯")["æ‰§è¡Œæ—¶é—´(ç§’)"]
                st.bar_chart(time_chart, use_container_width=True)
                
                fastest = df.loc[df["æ‰§è¡Œæ—¶é—´(ç§’)"].idxmin(), "RAGæŠ€æœ¯"]
                fastest_time = df["æ‰§è¡Œæ—¶é—´(ç§’)"].min()
                st.caption(f"âš¡ æœ€å¿«: {fastest} ({fastest_time:.2f}ç§’)")
            
            with col2:
                if "æ£€ç´¢ç²¾ç¡®åº¦" in df.columns:
                    st.markdown("**æ£€ç´¢è´¨é‡å¯¹æ¯”**")
                    retrieval_chart = df.set_index("RAGæŠ€æœ¯")["æ£€ç´¢ç²¾ç¡®åº¦"]
                    st.bar_chart(retrieval_chart, use_container_width=True)
                    
                    best_retrieval = df.loc[df["æ£€ç´¢ç²¾ç¡®åº¦"].idxmax(), "RAGæŠ€æœ¯"]
                    best_ret_score = df["æ£€ç´¢ç²¾ç¡®åº¦"].max()
                    st.caption(f"ğŸ¯ æœ€ä¼˜: {best_retrieval} ({best_ret_score:.2f})")
        
        # æ¨è
        st.markdown("#### ğŸ’¡ æ¨è")
        
        # ç»¼åˆè¯„åˆ†å‰3
        top3 = df.nlargest(3, "ç»¼åˆå¾—åˆ†")[["RAGæŠ€æœ¯", "ç»¼åˆå¾—åˆ†", "æ‰§è¡Œæ—¶é—´(ç§’)"]]
        st.markdown("**ç»¼åˆè¯„åˆ†Top 3:**")
        for idx, row in top3.iterrows():
            st.write(f"ğŸ¥‡ **{row['RAGæŠ€æœ¯']}** - å¾—åˆ†: {row['ç»¼åˆå¾—åˆ†']:.1f}/10, è€—æ—¶: {row['æ‰§è¡Œæ—¶é—´(ç§’)']:.2f}ç§’")
    
    else:
        st.warning("è¯„ä¼°æ•°æ®ä¸å®Œæ•´ï¼Œè¯·é‡æ–°è¯„ä¼°")


def batch_evaluate_all():
    """æ‰¹é‡è¯„ä¼°æ‰€æœ‰RAGæŠ€æœ¯"""
    
    eval_config = st.session_state.get("eval_config", {"auto_eval_enabled": True, "use_ragas": False})
    use_ragas = eval_config.get("use_ragas", False)
    
    # æ”¶é›†æ‰€æœ‰qa_record_id
    qa_record_ids = []
    for result in st.session_state.rag_results:
        if result.get("qa_record_id"):
            qa_record_ids.append(result["qa_record_id"])
    
    if not qa_record_ids:
        st.error("æ— æ³•è·å–QAè®°å½•IDï¼Œè¯·é‡æ–°æŸ¥è¯¢")
        return
    
    # æ˜¾ç¤ºè¿›åº¦
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # åˆå§‹åŒ–è¯„ä¼°ç»“æœ
        if "eval_results" not in st.session_state:
            st.session_state.eval_results = {}
        
        # é€ä¸ªè¯„ä¼°
        for i, qa_id in enumerate(qa_record_ids):
            status_text.text(f"æ­£åœ¨è¯„ä¼° {i+1}/{len(qa_record_ids)}: {st.session_state.rag_results[i]['rag_technique']}")
            
            try:
                response = requests.post(
                    f"{API_BASE_URL}/evaluation/auto",
                    json={
                        "qa_record_id": qa_id,
                        "use_llm_evaluator": True,
                        "use_ragas": use_ragas,
                        "reference_answer": None
                    },
                    timeout=120
                )
                
                if response.status_code == 200:
                    eval_result = response.json()
                    if eval_result.get("evaluation_success"):
                        st.session_state.eval_results[i] = eval_result
                    else:
                        st.warning(f"è¯„ä¼°å¤±è´¥: {st.session_state.rag_results[i]['rag_technique']}")
                else:
                    st.warning(f"APIé”™è¯¯: {response.status_code}")
                    
            except Exception as e:
                st.warning(f"è¯„ä¼°å‡ºé”™: {str(e)}")
            
            # æ›´æ–°è¿›åº¦
            progress_bar.progress((i + 1) / len(qa_record_ids))
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        status_text.text("âœ… è¯„ä¼°å®Œæˆ!")
        time.sleep(1)
        status_text.empty()
        progress_bar.empty()
        
        st.success(f"âœ… æˆåŠŸè¯„ä¼° {len(st.session_state.eval_results)}/{len(qa_record_ids)} ä¸ªRAGæŠ€æœ¯")
        st.rerun()
        
    except Exception as e:
        st.error(f"æ‰¹é‡è¯„ä¼°å¤±è´¥: {str(e)}")
        progress_bar.empty()
        status_text.empty()
