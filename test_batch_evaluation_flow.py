"""
æ‰¹é‡è¯„ä¼°æµç¨‹æµ‹è¯•è„šæœ¬

åŠŸèƒ½ï¼š
1. æ¨¡æ‹Ÿå‰ç«¯æ‰¹é‡è¯„ä¼°æµç¨‹
2. æµ‹è¯•å¤šä¸ªRAGæŠ€æœ¯çš„è¯„ä¼°
3. å±•ç¤ºç»Ÿè®¡å¯¹æ¯”åŠŸèƒ½

ä½¿ç”¨æ–¹æ³•ï¼š
    python test_batch_evaluation_flow.py
"""

import requests
import time
import json
from typing import List, Dict, Any


API_BASE_URL = "http://localhost:8000/api/v1"


def test_batch_evaluation_flow():
    """æµ‹è¯•æ‰¹é‡è¯„ä¼°å®Œæ•´æµç¨‹"""
    
    print("=" * 80)
    print("ğŸš€ æ‰¹é‡è¯„ä¼°æµç¨‹æµ‹è¯•")
    print("=" * 80)
    print()
    
    # æ­¥éª¤1: æŸ¥è¯¢å¤šä¸ªRAGæŠ€æœ¯
    print("ğŸ“ æ­¥éª¤1: æ‰§è¡ŒRAGæŸ¥è¯¢")
    print("-" * 80)
    
    query_request = {
        "query": "ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ",
        "document_ids": [1],  # å‡è®¾æ–‡æ¡£IDä¸º1
        "rag_techniques": [
            "simple_rag",
            "reranker_rag",
            "fusion_rag",
            "hyde_rag",
        ],
        "session_id": None,
        "llm_config": {"temperature": 0.7},
        "rag_config": {"top_k": 5}
    }
    
    try:
        print(f"ğŸ“¤ å‘é€æŸ¥è¯¢: {query_request['query']}")
        print(f"ğŸ¯ é€‰æ‹©RAGæŠ€æœ¯: {', '.join(query_request['rag_techniques'])}")
        print()
        
        response = requests.post(
            f"{API_BASE_URL}/qa/query",
            json=query_request,
            timeout=120
        )
        
        if response.status_code != 200:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {response.status_code}")
            print(response.text)
            return
        
        results = response.json()
        print(f"âœ… æŸ¥è¯¢æˆåŠŸ! è·å¾— {len(results)} ä¸ªRAGç»“æœ")
        print()
        
        # æå–qa_record_id
        qa_record_ids = []
        for i, result in enumerate(results):
            technique = result.get("rag_technique")
            qa_id = result.get("qa_record_id")
            exec_time = result.get("execution_time", 0)
            
            print(f"  {i+1}. {technique:30s} - è€—æ—¶: {exec_time:.2f}ç§’, QA_ID: {qa_id}")
            
            if qa_id:
                qa_record_ids.append(qa_id)
        
        print()
        
        if not qa_record_ids:
            print("âŒ æœªè·å–åˆ°qa_record_idï¼Œæ— æ³•ç»§ç»­è¯„ä¼°")
            return
        
        # æ­¥éª¤2: æ‰¹é‡è‡ªåŠ¨è¯„ä¼°
        print("=" * 80)
        print("ğŸ¤– æ­¥éª¤2: æ‰¹é‡è‡ªåŠ¨è¯„ä¼°")
        print("-" * 80)
        print()
        
        eval_results = []
        
        for i, qa_id in enumerate(qa_record_ids):
            technique = results[i]["rag_technique"]
            print(f"â³ è¯„ä¼°ä¸­ [{i+1}/{len(qa_record_ids)}]: {technique}")
            
            eval_request = {
                "qa_record_id": qa_id,
                "use_llm_evaluator": True,
                "use_ragas": False,  # å¿«é€Ÿæµ‹è¯•ï¼Œä¸ä½¿ç”¨Ragas
                "reference_answer": None
            }
            
            try:
                eval_response = requests.post(
                    f"{API_BASE_URL}/evaluation/auto",
                    json=eval_request,
                    timeout=120
                )
                
                if eval_response.status_code == 200:
                    eval_data = eval_response.json()
                    
                    if eval_data.get("evaluation_success"):
                        eval_results.append({
                            "technique": technique,
                            "qa_id": qa_id,
                            "eval_data": eval_data,
                            "exec_time": results[i]["execution_time"]
                        })
                        
                        llm_eval = eval_data.get("llm_evaluation", {})
                        overall = llm_eval.get("overall_score", 0)
                        print(f"  âœ… ç»¼åˆå¾—åˆ†: {overall:.1f}/10")
                    else:
                        print(f"  âš ï¸ è¯„ä¼°å¤±è´¥")
                else:
                    print(f"  âŒ APIé”™è¯¯: {eval_response.status_code}")
            
            except Exception as e:
                print(f"  âŒ è¯„ä¼°å‡ºé”™: {str(e)}")
            
            print()
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # æ­¥éª¤3: å±•ç¤ºç»Ÿè®¡å¯¹æ¯”
        print("=" * 80)
        print("ğŸ“Š æ­¥éª¤3: ç»Ÿè®¡å¯¹æ¯”ç»“æœ")
        print("=" * 80)
        print()
        
        if not eval_results:
            print("âŒ æ— å¯ç”¨çš„è¯„ä¼°ç»“æœ")
            return
        
        # æ„å»ºå¯¹æ¯”è¡¨æ ¼
        print("ğŸ“‹ è¯¦ç»†å¯¹æ¯”è¡¨:")
        print("-" * 100)
        header = f"{'RAGæŠ€æœ¯':<30s} {'æ‰§è¡Œæ—¶é—´':<12s} {'ç›¸å…³æ€§':<8s} {'å¿ å®åº¦':<8s} {'è¿è´¯æ€§':<8s} {'æµç•…åº¦':<8s} {'ç®€æ´æ€§':<8s} {'ç»¼åˆå¾—åˆ†':<10s}"
        print(header)
        print("-" * 100)
        
        comparison_data = []
        
        for result in eval_results:
            technique = result["technique"]
            exec_time = result["exec_time"]
            llm_eval = result["eval_data"].get("llm_evaluation", {})
            
            relevance = llm_eval.get("relevance_score", 0)
            faithfulness = llm_eval.get("faithfulness_score", 0)
            coherence = llm_eval.get("coherence_score", 0)
            fluency = llm_eval.get("fluency_score", 0)
            conciseness = llm_eval.get("conciseness_score", 0)
            overall = llm_eval.get("overall_score", 0)
            
            row = f"{technique:<30s} {exec_time:<12.2f} {relevance:<8.1f} {faithfulness:<8.1f} {coherence:<8.1f} {fluency:<8.1f} {conciseness:<8.1f} {overall:<10.1f}"
            print(row)
            
            comparison_data.append({
                "technique": technique,
                "exec_time": exec_time,
                "overall": overall,
                "relevance": relevance,
                "faithfulness": faithfulness,
                "coherence": coherence,
                "fluency": fluency,
                "conciseness": conciseness
            })
        
        print("-" * 100)
        print()
        
        # æ’å
        print("ğŸ† ç»¼åˆæ’å:")
        print("-" * 80)
        
        sorted_by_score = sorted(comparison_data, key=lambda x: x["overall"], reverse=True)
        
        medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"]
        for i, data in enumerate(sorted_by_score[:3]):
            medal = medals[i] if i < 3 else f"  {i+1}."
            print(f"{medal} {data['technique']:30s} - å¾—åˆ†: {data['overall']:5.1f}/10, è€—æ—¶: {data['exec_time']:5.2f}ç§’")
        
        print()
        
        # æœ€å¿«RAG
        fastest = min(comparison_data, key=lambda x: x["exec_time"])
        print(f"âš¡ æœ€å¿«RAG: {fastest['technique']} ({fastest['exec_time']:.2f}ç§’)")
        print()
        
        # å„ç»´åº¦æœ€ä¼˜
        print("ğŸ¯ å„ç»´åº¦æœ€ä¼˜:")
        print("-" * 80)
        
        dimensions = [
            ("ç›¸å…³æ€§", "relevance"),
            ("å¿ å®åº¦", "faithfulness"),
            ("è¿è´¯æ€§", "coherence"),
            ("æµç•…åº¦", "fluency"),
            ("ç®€æ´æ€§", "conciseness")
        ]
        
        for dim_name, dim_key in dimensions:
            best = max(comparison_data, key=lambda x: x[dim_key])
            print(f"  â€¢ {dim_name:8s}: {best['technique']:30s} ({best[dim_key]:.1f}åˆ†)")
        
        print()
        
        # ç»¼åˆåˆ†æ
        print("=" * 80)
        print("ğŸ’¡ ç»¼åˆåˆ†æä¸æ¨è")
        print("=" * 80)
        print()
        
        best_technique = sorted_by_score[0]
        
        print(f"ğŸ† æœ€ä½³ç»¼åˆè¡¨ç°: {best_technique['technique']}")
        print(f"   - ç»¼åˆå¾—åˆ†: {best_technique['overall']:.1f}/10")
        print(f"   - æ‰§è¡Œæ—¶é—´: {best_technique['exec_time']:.2f}ç§’")
        print(f"   - ç›¸å…³æ€§: {best_technique['relevance']:.1f}, å¿ å®åº¦: {best_technique['faithfulness']:.1f}")
        print()
        
        # å¹³è¡¡æ¨è
        efficiency_scores = [
            {
                "technique": d["technique"],
                "score": d["overall"],
                "time": d["exec_time"],
                "efficiency": d["overall"] / d["exec_time"]  # å¾—åˆ†/æ—¶é—´ = æ•ˆç‡
            }
            for d in comparison_data
        ]
        
        best_efficiency = max(efficiency_scores, key=lambda x: x["efficiency"])
        print(f"âš¡ æœ€ä½³æ•ˆç‡: {best_efficiency['technique']}")
        print(f"   - æ•ˆç‡å€¼: {best_efficiency['efficiency']:.2f} (å¾—åˆ†/ç§’)")
        print(f"   - ç»¼åˆå¾—åˆ†: {best_efficiency['score']:.1f}/10")
        print(f"   - æ‰§è¡Œæ—¶é—´: {best_efficiency['time']:.2f}ç§’")
        print()
        
        print("=" * 80)
        print("âœ… æ‰¹é‡è¯„ä¼°æµç¨‹æµ‹è¯•å®Œæˆ!")
        print("=" * 80)
        
    except requests.exceptions.ConnectionError:
        print("âŒ è¿æ¥å¤±è´¥: è¯·ç¡®ä¿åç«¯æœåŠ¡å·²å¯åŠ¨ (http://localhost:8000)")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print()
    print("ğŸ”§ æ‰¹é‡è¯„ä¼°æµç¨‹æµ‹è¯•è„šæœ¬")
    print("ğŸ“ ç¡®ä¿åç«¯æœåŠ¡å·²å¯åŠ¨: python backend/main.py")
    print("ğŸ“ ç¡®ä¿å·²ä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ–‡æ¡£")
    print()
    input("æŒ‰Enteré”®å¼€å§‹æµ‹è¯•...")
    print()
    
    test_batch_evaluation_flow()

