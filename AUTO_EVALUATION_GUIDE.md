# 自动化评估系统使用指南

## 📝 概述

本系统提供了完整的自动化RAG评估能力，替代人工评分，支持：
- **LLM评分器**: 使用Qwen-Plus进行多维度智能评分
- **Ragas框架**: 标准化RAG评估指标
- **批量评估**: 高效评估大量QA记录
- **自动保存**: 评估结果自动持久化

---

## 🎯 评估维度

### LLM评分器（5个维度）

#### 1. 相关性 (Relevance) 
**评估**: 答案与问题的相关程度
**评分标准**:
- 10分: 完美回答问题，完全相关
- 7-9分: 回答了问题的主要部分，高度相关
- 4-6分: 部分回答了问题，中等相关
- 1-3分: 基本没有回答问题，弱相关
- 0分: 完全无关

#### 2. 忠实度 (Faithfulness)
**评估**: 答案是否基于检索的文档
**评分标准**:
- 10分: 答案完全基于上下文，没有任何额外信息
- 7-9分: 答案主要基于上下文，有少量合理推断
- 4-6分: 答案部分基于上下文，部分来自外部知识
- 1-3分: 答案大部分不基于上下文
- 0分: 答案完全不基于上下文或与上下文矛盾

#### 3. 连贯性 (Coherence)
**评估**: 答案的逻辑连贯性和结构性
**评分标准**:
- 10分: 逻辑非常清晰，结构完美，前后一致
- 7-9分: 逻辑清晰，结构良好，基本一致
- 4-6分: 逻辑基本清晰，结构一般
- 1-3分: 逻辑混乱，结构不清
- 0分: 完全不连贯

#### 4. 流畅度 (Fluency)
**评估**: 语言流畅度和可读性
**评分标准**:
- 10分: 语言非常流畅，阅读体验极佳
- 7-9分: 语言流畅，易于阅读
- 4-6分: 语言基本流畅，有少量瑕疵
- 1-3分: 语言不够流畅，阅读困难
- 0分: 语言混乱，无法阅读

#### 5. 简洁性 (Conciseness)
**评估**: 答案是否简洁明了，没有冗余
**评分标准**:
- 10分: 非常简洁，没有任何冗余
- 7-9分: 比较简洁，少量冗余
- 4-6分: 基本简洁，有一定冗余
- 1-3分: 冗余较多，不够简洁
- 0分: 极度冗长，大量冗余


### Ragas评估（4-6个指标）

#### 1. Faithfulness
**定义**: 答案中的陈述是否能从给定的上下文中推断出来
**范围**: 0-1，越接近1越好

#### 2. Answer Relevancy
**定义**: 答案与问题的相关性
**范围**: 0-1，越接近1越好

#### 3. Context Precision
**定义**: 相关上下文在检索结果中的排序质量
**范围**: 0-1，越接近1越好

#### 4. Context Recall
**定义**: 上下文是否包含回答问题所需的所有信息
**范围**: 0-1，越接近1越好

#### 5. Answer Similarity (如果有参考答案)
**定义**: 生成答案与参考答案的相似度
**范围**: 0-1，越接近1越好

#### 6. Answer Correctness (如果有参考答案)
**定义**: 答案的准确性和完整性
**范围**: 0-1，越接近1越好


### 检索质量评估

#### 1. Retrieval Precision
**定义**: 检索到的相关文档占比
**计算**: 相关文档数 / 总检索文档数

#### 2. Context Relevance
**定义**: 检索文档的平均相关性
**范围**: 0-10

#### 3. Average Similarity
**定义**: 向量相似度的平均值
**范围**: 0-1

---

## 🚀 使用方式

### 方式1: API调用

#### 单条评估
```bash
curl -X POST "http://localhost:8000/api/v1/evaluation/auto" \
  -H "Content-Type: application/json" \
  -d '{
    "qa_record_id": 1,
    "use_llm_evaluator": true,
    "use_ragas": false,
    "reference_answer": null
  }'
```

#### 批量评估
```bash
curl -X POST "http://localhost:8000/api/v1/evaluation/auto/batch" \
  -H "Content-Type: application/json" \
  -d '{
    "qa_record_ids": [1, 2, 3, 4, 5],
    "use_llm_evaluator": true,
    "use_ragas": false
  }'
```

### 方式2: Python脚本
```bash
python test_auto_evaluation.py
```

### 方式3: 在前端界面使用（即将支持）
1. 在RAG结果对比面板找到"自动评估"按钮
2. 选择评估方式（LLM/Ragas/两者）
3. 点击评估，等待结果
4. 查看详细评分和反馈

---

## 📊 评估结果说明

### LLM评估结果示例
```json
{
  "relevance_score": 8.5,
  "faithfulness_score": 9.0,
  "coherence_score": 8.0,
  "fluency_score": 9.0,
  "conciseness_score": 7.5,
  "overall_score": 8.4,
  "feedback": "✅ 整体表现优秀 | 优势：忠实度优秀(9.0), 流畅度优秀(9.0)"
}
```

**解读**:
- 综合得分8.4/10，表现优秀
- 忠实度和流畅度是主要优势
- 简洁性略低，可能有冗余信息

### Ragas评估结果示例
```json
{
  "faithfulness": 0.92,
  "answer_relevancy": 0.87,
  "context_precision": 0.78,
  "context_recall": 0.85,
  "average_score": 0.855
}
```

**解读**:
- Faithfulness高(0.92)，答案基于文档
- Context Precision较低(0.78)，检索排序有优化空间
- 整体平均0.855，质量良好

---

## ⚙️ 配置说明

### LLM评分器配置

评分器使用Qwen-Plus模型，配置在 `backend/core/auto_evaluator.py`:

```python
evaluator = AutoEvaluator(
    llm_base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    llm_api_key="sk-e96412163b6a4f6189b65b98532eaf77",
    llm_model="qwen-plus"
)
```

**修改方法**:
1. 直接修改 `auto_evaluator.py` 中的默认值
2. 或者传入自定义配置

### Ragas配置

Ragas使用系统LLM配置，确保 `.env` 文件中配置了：
```ini
LLM_BASE_URL=your_llm_url
LLM_API_KEY=your_api_key
LLM_MODEL=your_model
```

---

## 💡 使用建议

### 1. 选择评估方式

**仅LLM评估**:
- ✅ 速度快（5-10秒/条）
- ✅ 多维度详细反馈
- ✅ 适合大批量评估
- ❌ 非标准化指标

**LLM + Ragas**:
- ✅ 标准化指标
- ✅ 更全面的评估
- ❌ 速度较慢（15-30秒/条）
- ❌ 需要额外依赖

**建议**:
- 日常评估: 仅LLM
- 重要对比: LLM + Ragas
- 大批量: 仅LLM

### 2. 批量vs单条

**批量评估**:
- 一次评估多条记录
- 总体耗时更少
- 适合定期评估

**单条评估**:
- 即时反馈
- 适合测试调试
- 可查看详细中间过程

### 3. 参考答案

**有参考答案时**:
- 可评估正确性
- Ragas可计算Answer Similarity和Correctness
- 更准确的质量判断

**无参考答案时**:
- 评估相对质量
- 基于内容本身判断
- 适合探索性问答

---

## 📈 性能指标

### 评估耗时

| 评估方式 | 单条耗时 | 批量耗时(10条) | 备注 |
|---------|---------|---------------|------|
| 仅LLM | 5-10秒 | 50-100秒 | 取决于答案长度 |
| LLM+Ragas | 15-30秒 | 150-300秒 | Ragas需要更多LLM调用 |

### LLM调用次数

| 评估维度 | LLM调用 |
|---------|---------|
| 相关性 | 1次 |
| 忠实度 | 1次 |
| 连贯性 | 1次 |
| 流畅度 | 1次 |
| 简洁性 | 1次 |
| 检索评估 | 1-3次 |
| **总计** | **6-8次** |

### 成本估算

假设Qwen-Plus价格：
- 输入：0.006元/1K tokens
- 输出：0.018元/1K tokens

单条评估成本约：**0.05-0.10元**

---

## 🔧 常见问题

### Q1: 评估很慢怎么办？
**A**: 
1. 使用批量评估而非逐条评估
2. 关闭Ragas评估
3. 减少检索文档数量
4. 使用更快的LLM模型

### Q2: Ragas评估失败？
**A**:
1. 检查是否安装ragas: `pip install ragas==0.1.7`
2. 检查LLM配置是否正确
3. 查看后端日志了解详细错误
4. 确保检索到了文档

### Q3: 评分结果不合理？
**A**:
1. LLM评分有一定主观性
2. 可以调整评分提示词
3. 多次评估取平均
4. 结合Ragas标准化指标

### Q4: 如何自定义评估维度？
**A**:
修改 `auto_evaluator.py`:
1. 添加新的 `_evaluate_xxx()` 方法
2. 在 `evaluate_answer()` 中调用
3. 更新 `final_scores` 字典

### Q5: 评估结果保存在哪里？
**A**:
- 数据库: `evaluations` 表
- 包含: 各维度评分、综合评分、反馈
- 可通过API查询: `/api/v1/evaluation/qa_record/{qa_record_id}`

---

## 📊 评估实践案例

### 案例1: 对比不同RAG技术

**场景**: 评估Simple RAG vs Adaptive RAG

**步骤**:
1. 使用同一问题分别生成答案
2. 对两条QA记录进行自动评估
3. 对比评分结果

**结果示例**:
```
Simple RAG:
- 综合得分: 7.2
- 相关性: 8.0
- 忠实度: 7.5
- 简洁性: 6.5

Adaptive RAG:
- 综合得分: 8.4
- 相关性: 9.0
- 忠实度: 8.5
- 简洁性: 7.5

结论: Adaptive RAG 在各维度都更优
```

### 案例2: 批量评估新技术

**场景**: 评估新实现的Self RAG在100个问题上的表现

**步骤**:
1. 准备100个测试问题
2. 使用Self RAG生成答案
3. 批量自动评估
4. 分析统计结果

**代码**:
```python
qa_ids = list(range(1, 101))  # 100条记录
result = batch_auto_evaluate(qa_ids)
```

**分析**:
- 平均综合得分: 8.2
- 忠实度优秀: 平均9.1
- 简洁性较弱: 平均6.8
- 建议: 优化答案长度控制

### 案例3: 持续监控

**场景**: 每日自动评估新增的QA记录

**实现**:
1. 创建定时任务
2. 每日获取新增QA记录
3. 批量自动评估
4. 生成日报

**伪代码**:
```python
def daily_evaluation():
    # 获取昨天的QA记录
    qa_ids = get_qa_records_yesterday()
    
    # 批量评估
    result = batch_auto_evaluate(qa_ids)
    
    # 生成报告
    generate_report(result)
    
    # 发送通知
    send_notification(result)
```

---

## 🎯 最佳实践

### 1. 评估流程

```
生成答案 → 自动评估 → 分析结果 → 优化策略 → 再次评估
```

### 2. 评分解读

- **8-10分**: 优秀，可直接使用
- **6-8分**: 良好，可优化
- **4-6分**: 一般，需改进
- **<4分**: 较差，重新设计

### 3. 优化策略

**相关性低**:
- 检查检索策略
- 优化查询转换
- 增加文档覆盖

**忠实度低**:
- 减少LLM幻觉
- 强调基于文档回答
- 使用Self RAG

**连贯性低**:
- 优化Prompt
- 调整LLM参数
- 后处理优化

**流畅度低**:
- 改进生成Prompt
- 使用更好的LLM
- 人工后编辑

**简洁性低**:
- 限制生成长度
- 使用压缩技术
- 优化摘要能力

---

## 📝 总结

### 核心价值

1. **自动化**: 替代人工评分，节省大量时间
2. **多维度**: 5+4维度全面评估
3. **标准化**: Ragas提供行业标准指标
4. **可扩展**: 易于添加新评估维度
5. **持久化**: 自动保存，便于分析

### 适用场景

- ✅ RAG技术对比评估
- ✅ 系统性能监控
- ✅ 答案质量保证
- ✅ 批量内容评审
- ✅ 优化效果验证

### 未来优化

- [ ] 支持自定义评估维度
- [ ] 添加更多Ragas指标
- [ ] 实现评估结果可视化
- [ ] 支持评估模板配置
- [ ] 集成更多评估框架

---

**更新时间**: 2025-10-13
**版本**: V1.3
**作者**: AI Assistant

如有问题或建议，请提交Issue。

