# 前端自动评估功能使用指南

## 📝 概述

前端现已支持一键自动评估功能，替代了之前的手动评分滑块。

---

## 🎯 功能特点

### 新增功能
- ✅ **一键自动评估**: 点击按钮即可自动评估答案质量
- ✅ **多维度展示**: 5维度LLM评分 + Ragas标准化指标
- ✅ **检索质量评估**: 自动评估检索精确度和上下文相关性
- ✅ **智能反馈**: 根据评分自动生成改进建议
- ✅ **可选Ragas**: 根据需要选择是否使用Ragas评估

### 替代功能
- ❌ 移除了手动评分滑块（准确性、流畅性、相关性、完整性）
- ❌ 移除了手动评价说明输入框
- ❌ 移除了"提交评分"按钮

---

## 🚀 使用步骤

### 1. 提出问题
在中间对话窗口输入问题，系统会使用选定的RAG技术生成答案。

### 2. 查看结果
在右侧"📊 RAG结果对比"面板，切换到要评估的RAG技术标签。

### 3. 开始评估

#### 步骤1: 选择评估方式
- 默认: 仅LLM评估（快速，5-10秒）
- 勾选"使用Ragas评估": LLM + Ragas（标准，15-30秒）

#### 步骤2: 点击评估按钮
点击 **"🚀 开始评估"** 按钮

#### 步骤3: 等待结果
- 显示"正在评估中..."进度条
- 评估完成后显示"✅ 评估完成! 耗时: X秒"

### 4. 查看评估结果

#### LLM评分 (0-10分)
```
相关性    忠实度
连贯性    流畅度
简洁性    综合得分
```

#### 智能反馈
```
💡 ✅ 整体表现优秀 | 优势：忠实度优秀(9.0), 流畅度优秀(9.0)
```

#### Ragas评估 (0-1分) - 如果勾选
```
Faithfulness  Answer Rel.  Context Pre.  Context Rec.
```

#### 检索质量
```
检索精确度      上下文相关性
```

---

## 📊 评分说明

### LLM评分维度

#### 相关性 (Relevance)
- 答案与问题的相关程度
- 10分: 完美回答
- 0分: 完全无关

#### 忠实度 (Faithfulness)
- 答案是否基于检索的文档
- 10分: 完全基于文档
- 0分: 与文档无关或矛盾

#### 连贯性 (Coherence)
- 答案的逻辑连贯性和结构性
- 10分: 逻辑清晰，结构完美
- 0分: 完全不连贯

#### 流畅度 (Fluency)
- 语言流畅度和可读性
- 10分: 语言流畅，阅读体验极佳
- 0分: 语言混乱，无法阅读

#### 简洁性 (Conciseness)
- 是否简洁明了，没有冗余
- 10分: 非常简洁
- 0分: 极度冗长

#### 综合得分
- 5个维度的平均分
- 用于快速判断整体质量

### Ragas指标 (0-1)

- **Faithfulness**: 答案的忠实度
- **Answer Relevancy**: 答案相关性
- **Context Precision**: 上下文精确度
- **Context Recall**: 上下文召回率

### 检索质量

- **检索精确度**: 相关文档占比（0-1）
- **上下文相关性**: 检索文档的平均相关性（0-10）

---

## 💡 使用建议

### 何时使用LLM评估
- ✅ 日常快速评估
- ✅ 需要详细反馈
- ✅ 对比多个RAG技术

### 何时使用Ragas
- ✅ 正式评估报告
- ✅ 学术研究
- ✅ 需要标准化指标
- ⚠️ 注意：评估时间更长

### 评分解读

| 综合得分 | 评价 | 建议 |
|---------|------|------|
| 8-10分 | 优秀 | 可直接使用 |
| 6-8分 | 良好 | 可优化 |
| 4-6分 | 一般 | 需改进 |
| <4分 | 较差 | 重新设计 |

---

## 🎨 界面变化对比

### 之前（手动评分）
```
⭐ 评分

准确性 ━━━━━●━━━━ 5.00
流畅性 ━━━━━●━━━━ 5.00
相关性 ━━━━━●━━━━ 5.00
完整性 ━━━━━●━━━━ 5.00
总体评分 ━━━━━●━━━━ 5.00

评价说明
[文本输入框]

[提交评分]
```

### 现在（自动评估）
```
🤖 自动评估

☐ 使用Ragas评估（更慢但更标准）  [🚀 开始评估]

📊 评估结果

LLM评分 (0-10分)
相关性  8.5    连贯性  8.0    简洁性  7.5
忠实度  9.0    流畅度  9.0    综合得分 8.4

💡 ✅ 整体表现优秀 | 优势：忠实度优秀(9.0), 流畅度优秀(9.0)

检索质量
检索精确度  0.67    上下文相关性  8.2/10
```

---

## ⚠️ 注意事项

### 1. 评估前提
- 必须先生成答案才能评估
- 确保已经提出问题并获得回答

### 2. 评估时间
- LLM评估: 5-10秒
- LLM+Ragas: 15-30秒
- 请耐心等待，不要重复点击

### 3. 错误处理
如果评估失败:
- 检查后端服务是否正常
- 查看是否有QA记录ID
- 尝试重新提问

### 4. 会话状态
- 评估结果会保存在当前会话
- 刷新页面后需要重新评估
- 切换标签页不会丢失评估结果

---

## 🔧 技术细节

### 数据流程
```
用户提问
    ↓
生成答案 (保存QA记录ID)
    ↓
点击"开始评估"按钮
    ↓
调用 POST /api/v1/evaluation/auto
    ↓
后端LLM评分器评估
    ↓
(可选) Ragas评估
    ↓
返回评估结果
    ↓
前端展示评分和反馈
```

### API请求
```json
POST /api/v1/evaluation/auto
{
  "qa_record_id": 123,
  "use_llm_evaluator": true,
  "use_ragas": false,
  "reference_answer": null
}
```

### API响应
```json
{
  "qa_record_id": 123,
  "evaluation_success": true,
  "llm_evaluation": {
    "relevance_score": 8.5,
    "faithfulness_score": 9.0,
    "coherence_score": 8.0,
    "fluency_score": 9.0,
    "conciseness_score": 7.5,
    "overall_score": 8.4,
    "feedback": "✅ 整体表现优秀..."
  },
  "final_scores": {...},
  "evaluation_time": 8.5
}
```

---

## 📈 效果对比

### 手动评分 vs 自动评估

| 特性 | 手动评分 | 自动评估 |
|-----|---------|---------|
| 速度 | 1-2分钟 | 5-10秒 |
| 客观性 | 主观 | 客观 |
| 一致性 | 低 | 高 |
| 详细程度 | 简单 | 详细 |
| 标准化 | 否 | 是(Ragas) |
| 反馈 | 无 | 有 |

---

## 🎯 常见问题

### Q: 为什么移除了手动评分？
A: 自动评估更客观、快速、一致，能提供更详细的反馈。

### Q: 我还能手动调整分数吗？
A: 当前版本专注于自动评估。如需手动调整，可以通过API直接创建评分记录。

### Q: Ragas评估很慢，必须用吗？
A: 不必须。日常使用LLM评估即可，Ragas适合正式评估。

### Q: 评估结果会保存吗？
A: 是的，自动保存到数据库，可在统计对比中查看。

### Q: 如何对比多个RAG技术？
A: 对每个技术标签页点击评估，然后在"统计对比"查看汇总。

---

## 🚀 快速体验

1. 启动服务
```bash
./start_all.sh
```

2. 打开浏览器
```
http://localhost:8501
```

3. 上传文档 → 提出问题 → 切换到RAG对比标签 → 点击"开始评估"

4. 查看评分结果和反馈

---

**更新时间**: 2025-10-13  
**版本**: V1.3  
**功能**: 前端自动评估

🎉 享受自动化评估带来的便利！

