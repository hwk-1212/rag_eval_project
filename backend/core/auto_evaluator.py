"""
è‡ªåŠ¨åŒ–è¯„ä¼°æ¨¡å—
ç»“åˆLLMè¯„åˆ†å’ŒRagasæ¡†æ¶è¿›è¡Œå¤šç»´åº¦RAGè¯„ä¼°
"""
from typing import List, Dict, Any, Optional
from loguru import logger
import re
import json
from openai import OpenAI


class AutoEvaluator:
    """è‡ªåŠ¨åŒ–è¯„ä¼°å™¨"""
    
    def __init__(
        self,
        llm_base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
        llm_api_key: str = "sk-e96412163b6a4f6189b65b98532eaf77",
        llm_model: str = "qwen-plus"
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            llm_base_url: è¯„åˆ†æ¨¡å‹çš„APIåœ°å€
            llm_api_key: APIå¯†é’¥
            llm_model: è¯„åˆ†ä½¿ç”¨çš„æ¨¡å‹
        """
        self.client = OpenAI(
            base_url=llm_base_url,
            api_key=llm_api_key
        )
        self.model = llm_model
        logger.info(f"[AutoEvaluator] åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {llm_model}")
    
    def evaluate_answer(
        self,
        query: str,
        answer: str,
        retrieved_contexts: List[str],
        reference_answer: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å…¨é¢è¯„ä¼°RAGç­”æ¡ˆè´¨é‡
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            answer: RAGç”Ÿæˆçš„ç­”æ¡ˆ
            retrieved_contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
            reference_answer: å‚è€ƒç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        logger.info("[AutoEvaluator] å¼€å§‹è¯„ä¼°ç­”æ¡ˆ...")
        
        results = {
            # LLMè¯„åˆ†æŒ‡æ ‡
            "relevance_score": 0.0,        # ç›¸å…³æ€§
            "faithfulness_score": 0.0,     # å¿ å®åº¦
            "coherence_score": 0.0,        # è¿è´¯æ€§
            "fluency_score": 0.0,          # æµç•…åº¦
            "conciseness_score": 0.0,      # ç®€æ´æ€§
            
            # ç»¼åˆè¯„åˆ†
            "overall_score": 0.0,
            
            # è¯¦ç»†åé¦ˆ
            "feedback": "",
            
            # è¯„ä¼°çŠ¶æ€
            "evaluation_success": True,
            "error_message": None
        }
        
        try:
            # 1. ç›¸å…³æ€§è¯„ä¼°
            results["relevance_score"] = self._evaluate_relevance(query, answer)
            
            # 2. å¿ å®åº¦è¯„ä¼°ï¼ˆç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢å†…å®¹ï¼‰
            results["faithfulness_score"] = self._evaluate_faithfulness(
                answer, retrieved_contexts
            )
            
            # 3. è¿è´¯æ€§è¯„ä¼°
            results["coherence_score"] = self._evaluate_coherence(answer)
            
            # 4. æµç•…åº¦è¯„ä¼°
            results["fluency_score"] = self._evaluate_fluency(answer)
            
            # 5. ç®€æ´æ€§è¯„ä¼°
            results["conciseness_score"] = self._evaluate_conciseness(answer)
            
            # 6. å¦‚æœæœ‰å‚è€ƒç­”æ¡ˆï¼Œè¯„ä¼°æ­£ç¡®æ€§
            if reference_answer:
                results["correctness_score"] = self._evaluate_correctness(
                    answer, reference_answer
                )
            
            # 7. è®¡ç®—ç»¼åˆè¯„åˆ†
            scores = [
                results["relevance_score"],
                results["faithfulness_score"],
                results["coherence_score"],
                results["fluency_score"],
                results["conciseness_score"]
            ]
            results["overall_score"] = round(sum(scores) / len(scores), 2)
            
            # 8. ç”Ÿæˆåé¦ˆ
            results["feedback"] = self._generate_feedback(results)
            
            logger.info(f"[AutoEvaluator] è¯„ä¼°å®Œæˆï¼Œç»¼åˆå¾—åˆ†: {results['overall_score']}")
            
        except Exception as e:
            logger.error(f"[AutoEvaluator] è¯„ä¼°å¤±è´¥: {e}")
            results["evaluation_success"] = False
            results["error_message"] = str(e)
        
        return results
    
    def _evaluate_relevance(self, query: str, answer: str) -> float:
        """
        è¯„ä¼°ç­”æ¡ˆä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
        
        Args:
            query: æŸ¥è¯¢
            answer: ç­”æ¡ˆ
            
        Returns:
            ç›¸å…³æ€§åˆ†æ•° (0-10)
        """
        try:
            system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„ç­”æ¡ˆè´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³æ€§ã€‚

è¯„åˆ†æ ‡å‡†ï¼ˆ0-10åˆ†ï¼‰ï¼š
- 10åˆ†ï¼šå®Œç¾å›ç­”é—®é¢˜ï¼Œå®Œå…¨ç›¸å…³
- 7-9åˆ†ï¼šå›ç­”äº†é—®é¢˜çš„ä¸»è¦éƒ¨åˆ†ï¼Œé«˜åº¦ç›¸å…³
- 4-6åˆ†ï¼šéƒ¨åˆ†å›ç­”äº†é—®é¢˜ï¼Œä¸­ç­‰ç›¸å…³
- 1-3åˆ†ï¼šåŸºæœ¬æ²¡æœ‰å›ç­”é—®é¢˜ï¼Œå¼±ç›¸å…³
- 0åˆ†ï¼šå®Œå…¨æ— å…³

è¯·ä»…è¿”å›0-10ä¹‹é—´çš„å•ä¸ªæ•°å­—ã€‚"""
            
            user_prompt = f"""é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼š{answer}

ç›¸å…³æ€§è¯„åˆ†ï¼ˆ0-10ï¼‰ï¼š"""
            
            score = self._call_llm_for_score(system_prompt, user_prompt)
            logger.debug(f"[Relevance] åˆ†æ•°: {score}")
            return score
            
        except Exception as e:
            logger.error(f"[Relevance] è¯„ä¼°å¤±è´¥: {e}")
            return 5.0
    
    def _evaluate_faithfulness(self, answer: str, contexts: List[str]) -> float:
        """
        è¯„ä¼°ç­”æ¡ˆçš„å¿ å®åº¦ï¼ˆæ˜¯å¦åŸºäºæ£€ç´¢å†…å®¹ï¼‰
        
        Args:
            answer: ç­”æ¡ˆ
            contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            
        Returns:
            å¿ å®åº¦åˆ†æ•° (0-10)
        """
        try:
            # åˆå¹¶ä¸Šä¸‹æ–‡
            combined_context = "\n\n".join(contexts[:3])  # åªç”¨å‰3ä¸ª
            
            # é™åˆ¶é•¿åº¦
            if len(combined_context) > 2000:
                combined_context = combined_context[:2000] + "..."
            
            system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„ç­”æ¡ˆå¿ å®åº¦è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ç­”æ¡ˆæ˜¯å¦åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ã€‚

è¯„åˆ†æ ‡å‡†ï¼ˆ0-10åˆ†ï¼‰ï¼š
- 10åˆ†ï¼šç­”æ¡ˆå®Œå…¨åŸºäºä¸Šä¸‹æ–‡ï¼Œæ²¡æœ‰ä»»ä½•é¢å¤–ä¿¡æ¯
- 7-9åˆ†ï¼šç­”æ¡ˆä¸»è¦åŸºäºä¸Šä¸‹æ–‡ï¼Œæœ‰å°‘é‡åˆç†æ¨æ–­
- 4-6åˆ†ï¼šç­”æ¡ˆéƒ¨åˆ†åŸºäºä¸Šä¸‹æ–‡ï¼Œéƒ¨åˆ†æ¥è‡ªå¤–éƒ¨çŸ¥è¯†
- 1-3åˆ†ï¼šç­”æ¡ˆå¤§éƒ¨åˆ†ä¸åŸºäºä¸Šä¸‹æ–‡
- 0åˆ†ï¼šç­”æ¡ˆå®Œå…¨ä¸åŸºäºä¸Šä¸‹æ–‡æˆ–ä¸ä¸Šä¸‹æ–‡çŸ›ç›¾

è¯·ä»…è¿”å›0-10ä¹‹é—´çš„å•ä¸ªæ•°å­—ã€‚"""
            
            user_prompt = f"""ä¸Šä¸‹æ–‡ï¼š
{combined_context}

ç­”æ¡ˆï¼š{answer}

å¿ å®åº¦è¯„åˆ†ï¼ˆ0-10ï¼‰ï¼š"""
            
            score = self._call_llm_for_score(system_prompt, user_prompt)
            logger.debug(f"[Faithfulness] åˆ†æ•°: {score}")
            return score
            
        except Exception as e:
            logger.error(f"[Faithfulness] è¯„ä¼°å¤±è´¥: {e}")
            return 5.0
    
    def _evaluate_coherence(self, answer: str) -> float:
        """
        è¯„ä¼°ç­”æ¡ˆçš„è¿è´¯æ€§
        
        Args:
            answer: ç­”æ¡ˆ
            
        Returns:
            è¿è´¯æ€§åˆ†æ•° (0-10)
        """
        try:
            system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„æ–‡æœ¬è¿è´¯æ€§è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ç­”æ¡ˆçš„é€»è¾‘è¿è´¯æ€§å’Œç»“æ„æ€§ã€‚

è¯„åˆ†æ ‡å‡†ï¼ˆ0-10åˆ†ï¼‰ï¼š
- 10åˆ†ï¼šé€»è¾‘éå¸¸æ¸…æ™°ï¼Œç»“æ„å®Œç¾ï¼Œå‰åä¸€è‡´
- 7-9åˆ†ï¼šé€»è¾‘æ¸…æ™°ï¼Œç»“æ„è‰¯å¥½ï¼ŒåŸºæœ¬ä¸€è‡´
- 4-6åˆ†ï¼šé€»è¾‘åŸºæœ¬æ¸…æ™°ï¼Œç»“æ„ä¸€èˆ¬
- 1-3åˆ†ï¼šé€»è¾‘æ··ä¹±ï¼Œç»“æ„ä¸æ¸…
- 0åˆ†ï¼šå®Œå…¨ä¸è¿è´¯

è¯·ä»…è¿”å›0-10ä¹‹é—´çš„å•ä¸ªæ•°å­—ã€‚"""
            
            user_prompt = f"""ç­”æ¡ˆï¼š{answer}

è¿è´¯æ€§è¯„åˆ†ï¼ˆ0-10ï¼‰ï¼š"""
            
            score = self._call_llm_for_score(system_prompt, user_prompt)
            logger.debug(f"[Coherence] åˆ†æ•°: {score}")
            return score
            
        except Exception as e:
            logger.error(f"[Coherence] è¯„ä¼°å¤±è´¥: {e}")
            return 5.0
    
    def _evaluate_fluency(self, answer: str) -> float:
        """
        è¯„ä¼°ç­”æ¡ˆçš„æµç•…åº¦
        
        Args:
            answer: ç­”æ¡ˆ
            
        Returns:
            æµç•…åº¦åˆ†æ•° (0-10)
        """
        try:
            system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„æ–‡æœ¬æµç•…åº¦è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ç­”æ¡ˆçš„è¯­è¨€æµç•…åº¦å’Œå¯è¯»æ€§ã€‚

è¯„åˆ†æ ‡å‡†ï¼ˆ0-10åˆ†ï¼‰ï¼š
- 10åˆ†ï¼šè¯­è¨€éå¸¸æµç•…ï¼Œé˜…è¯»ä½“éªŒæä½³
- 7-9åˆ†ï¼šè¯­è¨€æµç•…ï¼Œæ˜“äºé˜…è¯»
- 4-6åˆ†ï¼šè¯­è¨€åŸºæœ¬æµç•…ï¼Œæœ‰å°‘é‡ç‘•ç–µ
- 1-3åˆ†ï¼šè¯­è¨€ä¸å¤Ÿæµç•…ï¼Œé˜…è¯»å›°éš¾
- 0åˆ†ï¼šè¯­è¨€æ··ä¹±ï¼Œæ— æ³•é˜…è¯»

è¯·ä»…è¿”å›0-10ä¹‹é—´çš„å•ä¸ªæ•°å­—ã€‚"""
            
            user_prompt = f"""ç­”æ¡ˆï¼š{answer}

æµç•…åº¦è¯„åˆ†ï¼ˆ0-10ï¼‰ï¼š"""
            
            score = self._call_llm_for_score(system_prompt, user_prompt)
            logger.debug(f"[Fluency] åˆ†æ•°: {score}")
            return score
            
        except Exception as e:
            logger.error(f"[Fluency] è¯„ä¼°å¤±è´¥: {e}")
            return 5.0
    
    def _evaluate_conciseness(self, answer: str) -> float:
        """
        è¯„ä¼°ç­”æ¡ˆçš„ç®€æ´æ€§
        
        Args:
            answer: ç­”æ¡ˆ
            
        Returns:
            ç®€æ´æ€§åˆ†æ•° (0-10)
        """
        try:
            system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„æ–‡æœ¬ç®€æ´æ€§è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ç­”æ¡ˆæ˜¯å¦ç®€æ´æ˜äº†ï¼Œæ²¡æœ‰å†—ä½™ã€‚

è¯„åˆ†æ ‡å‡†ï¼ˆ0-10åˆ†ï¼‰ï¼š
- 10åˆ†ï¼šéå¸¸ç®€æ´ï¼Œæ²¡æœ‰ä»»ä½•å†—ä½™
- 7-9åˆ†ï¼šæ¯”è¾ƒç®€æ´ï¼Œå°‘é‡å†—ä½™
- 4-6åˆ†ï¼šåŸºæœ¬ç®€æ´ï¼Œæœ‰ä¸€å®šå†—ä½™
- 1-3åˆ†ï¼šå†—ä½™è¾ƒå¤šï¼Œä¸å¤Ÿç®€æ´
- 0åˆ†ï¼šæåº¦å†—é•¿ï¼Œå¤§é‡å†—ä½™

è¯·ä»…è¿”å›0-10ä¹‹é—´çš„å•ä¸ªæ•°å­—ã€‚"""
            
            user_prompt = f"""ç­”æ¡ˆï¼š{answer}

ç®€æ´æ€§è¯„åˆ†ï¼ˆ0-10ï¼‰ï¼š"""
            
            score = self._call_llm_for_score(system_prompt, user_prompt)
            logger.debug(f"[Conciseness] åˆ†æ•°: {score}")
            return score
            
        except Exception as e:
            logger.error(f"[Conciseness] è¯„ä¼°å¤±è´¥: {e}")
            return 5.0
    
    def _evaluate_correctness(self, answer: str, reference: str) -> float:
        """
        è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼ˆä¸å‚è€ƒç­”æ¡ˆå¯¹æ¯”ï¼‰
        
        Args:
            answer: ç­”æ¡ˆ
            reference: å‚è€ƒç­”æ¡ˆ
            
        Returns:
            æ­£ç¡®æ€§åˆ†æ•° (0-10)
        """
        try:
            system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„ç­”æ¡ˆæ­£ç¡®æ€§è¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹æ¯”ç­”æ¡ˆå’Œå‚è€ƒç­”æ¡ˆï¼Œè¯„ä¼°æ­£ç¡®æ€§ã€‚

è¯„åˆ†æ ‡å‡†ï¼ˆ0-10åˆ†ï¼‰ï¼š
- 10åˆ†ï¼šä¸å‚è€ƒç­”æ¡ˆå®Œå…¨ä¸€è‡´æˆ–æ›´ä¼˜
- 7-9åˆ†ï¼šä¸å‚è€ƒç­”æ¡ˆåŸºæœ¬ä¸€è‡´ï¼Œå°‘é‡å·®å¼‚
- 4-6åˆ†ï¼šéƒ¨åˆ†æ­£ç¡®ï¼Œæœ‰ä¸€å®šå·®å¼‚
- 1-3åˆ†ï¼šå¤§éƒ¨åˆ†ä¸æ­£ç¡®
- 0åˆ†ï¼šå®Œå…¨é”™è¯¯

è¯·ä»…è¿”å›0-10ä¹‹é—´çš„å•ä¸ªæ•°å­—ã€‚"""
            
            user_prompt = f"""å‚è€ƒç­”æ¡ˆï¼š{reference}

å¾…è¯„ä¼°ç­”æ¡ˆï¼š{answer}

æ­£ç¡®æ€§è¯„åˆ†ï¼ˆ0-10ï¼‰ï¼š"""
            
            score = self._call_llm_for_score(system_prompt, user_prompt)
            logger.debug(f"[Correctness] åˆ†æ•°: {score}")
            return score
            
        except Exception as e:
            logger.error(f"[Correctness] è¯„ä¼°å¤±è´¥: {e}")
            return 5.0
    
    def _call_llm_for_score(self, system_prompt: str, user_prompt: str) -> float:
        """
        è°ƒç”¨LLMè·å–è¯„åˆ†
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤º
            user_prompt: ç”¨æˆ·æç¤º
            
        Returns:
            è¯„åˆ† (0-10)
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                temperature=0,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )
            
            score_text = response.choices[0].message.content.strip()
            
            # æå–æ•°å­—
            match = re.search(r'(\d+(\.\d+)?)', score_text)
            if match:
                score = float(match.group(1))
                return min(10.0, max(0.0, score))  # ç¡®ä¿åœ¨0-10èŒƒå›´å†…
            else:
                logger.warning(f"[LLM Score] æ— æ³•æå–åˆ†æ•°: {score_text}")
                return 5.0
                
        except Exception as e:
            logger.error(f"[LLM Score] è°ƒç”¨å¤±è´¥: {e}")
            return 5.0
    
    def _generate_feedback(self, results: Dict[str, Any]) -> str:
        """
        æ ¹æ®è¯„åˆ†ç»“æœç”Ÿæˆåé¦ˆ
        
        Args:
            results: è¯„åˆ†ç»“æœ
            
        Returns:
            åé¦ˆæ–‡æœ¬
        """
        feedback_parts = []
        
        # æ•´ä½“è¯„ä»·
        overall = results["overall_score"]
        if overall >= 8:
            feedback_parts.append("âœ… æ•´ä½“è¡¨ç°ä¼˜ç§€")
        elif overall >= 6:
            feedback_parts.append("ğŸ‘ æ•´ä½“è¡¨ç°è‰¯å¥½")
        elif overall >= 4:
            feedback_parts.append("âš ï¸ æ•´ä½“è¡¨ç°ä¸€èˆ¬")
        else:
            feedback_parts.append("âŒ æ•´ä½“è¡¨ç°è¾ƒå·®")
        
        # å„é¡¹å…·ä½“è¯„ä»·
        metrics = {
            "relevance_score": "ç›¸å…³æ€§",
            "faithfulness_score": "å¿ å®åº¦",
            "coherence_score": "è¿è´¯æ€§",
            "fluency_score": "æµç•…åº¦",
            "conciseness_score": "ç®€æ´æ€§"
        }
        
        strengths = []
        weaknesses = []
        
        for key, name in metrics.items():
            score = results.get(key, 0)
            if score >= 8:
                strengths.append(f"{name}ä¼˜ç§€({score:.1f})")
            elif score < 5:
                weaknesses.append(f"{name}è¾ƒå¼±({score:.1f})")
        
        if strengths:
            feedback_parts.append(f"ä¼˜åŠ¿ï¼š{', '.join(strengths)}")
        if weaknesses:
            feedback_parts.append(f"éœ€æ”¹è¿›ï¼š{', '.join(weaknesses)}")
        
        return " | ".join(feedback_parts)
    
    def evaluate_retrieval(
        self,
        query: str,
        retrieved_contexts: List[str],
        context_scores: List[float]
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°æ£€ç´¢è´¨é‡
        
        Args:
            query: æŸ¥è¯¢
            retrieved_contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            context_scores: ç›¸ä¼¼åº¦åˆ†æ•°
            
        Returns:
            æ£€ç´¢è¯„ä¼°ç»“æœ
        """
        logger.info("[AutoEvaluator] å¼€å§‹è¯„ä¼°æ£€ç´¢è´¨é‡...")
        
        results = {
            "retrieval_precision": 0.0,    # æ£€ç´¢ç²¾ç¡®åº¦
            "context_relevance": 0.0,      # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
            "avg_similarity": 0.0,         # å¹³å‡ç›¸ä¼¼åº¦
            "retrieval_feedback": ""
        }
        
        try:
            # 1. å¹³å‡ç›¸ä¼¼åº¦
            if context_scores:
                results["avg_similarity"] = round(
                    sum(context_scores) / len(context_scores), 3
                )
            
            # 2. è¯„ä¼°æ¯ä¸ªä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§
            relevance_scores = []
            for i, context in enumerate(retrieved_contexts[:3]):  # åªè¯„ä¼°å‰3ä¸ª
                score = self._evaluate_context_relevance(query, context)
                relevance_scores.append(score)
            
            if relevance_scores:
                results["context_relevance"] = round(
                    sum(relevance_scores) / len(relevance_scores), 2
                )
            
            # 3. è®¡ç®—ç²¾ç¡®åº¦ï¼ˆç›¸å…³çš„å æ¯”ï¼‰
            relevant_count = sum(1 for s in relevance_scores if s >= 6)
            results["retrieval_precision"] = round(
                relevant_count / len(relevance_scores) if relevance_scores else 0, 2
            )
            
            # 4. ç”Ÿæˆåé¦ˆ
            if results["retrieval_precision"] >= 0.8:
                results["retrieval_feedback"] = "âœ… æ£€ç´¢è´¨é‡ä¼˜ç§€ï¼Œç›¸å…³æ–‡æ¡£å æ¯”é«˜"
            elif results["retrieval_precision"] >= 0.5:
                results["retrieval_feedback"] = "ğŸ‘ æ£€ç´¢è´¨é‡è‰¯å¥½ï¼Œå¤§éƒ¨åˆ†æ–‡æ¡£ç›¸å…³"
            else:
                results["retrieval_feedback"] = "âš ï¸ æ£€ç´¢è´¨é‡ä¸€èˆ¬ï¼Œéœ€ä¼˜åŒ–æ£€ç´¢ç­–ç•¥"
            
            logger.info(f"[AutoEvaluator] æ£€ç´¢è¯„ä¼°å®Œæˆï¼Œç²¾ç¡®åº¦: {results['retrieval_precision']}")
            
        except Exception as e:
            logger.error(f"[AutoEvaluator] æ£€ç´¢è¯„ä¼°å¤±è´¥: {e}")
        
        return results
    
    def _evaluate_context_relevance(self, query: str, context: str) -> float:
        """
        è¯„ä¼°å•ä¸ªä¸Šä¸‹æ–‡ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
        
        Args:
            query: æŸ¥è¯¢
            context: ä¸Šä¸‹æ–‡
            
        Returns:
            ç›¸å…³æ€§åˆ†æ•° (0-10)
        """
        try:
            # é™åˆ¶é•¿åº¦
            if len(context) > 1000:
                context = context[:1000] + "..."
            
            system_prompt = """ä½ æ˜¯æ–‡æ¡£ç›¸å…³æ€§è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°æ–‡æ¡£ç‰‡æ®µä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚

è¯„åˆ†æ ‡å‡†ï¼ˆ0-10åˆ†ï¼‰ï¼š
- 10åˆ†ï¼šå®Œç¾åŒ¹é…ï¼Œç›´æ¥å›ç­”æŸ¥è¯¢
- 7-9åˆ†ï¼šé«˜åº¦ç›¸å…³ï¼ŒåŒ…å«å…³é”®ä¿¡æ¯
- 4-6åˆ†ï¼šä¸­ç­‰ç›¸å…³ï¼ŒåŒ…å«éƒ¨åˆ†ä¿¡æ¯
- 1-3åˆ†ï¼šå¼±ç›¸å…³ï¼ŒåŒ…å«å°‘é‡ç›¸å…³ä¿¡æ¯
- 0åˆ†ï¼šå®Œå…¨æ— å…³

è¯·ä»…è¿”å›0-10ä¹‹é—´çš„å•ä¸ªæ•°å­—ã€‚"""
            
            user_prompt = f"""æŸ¥è¯¢ï¼š{query}

æ–‡æ¡£ç‰‡æ®µï¼š{context}

ç›¸å…³æ€§è¯„åˆ†ï¼ˆ0-10ï¼‰ï¼š"""
            
            return self._call_llm_for_score(system_prompt, user_prompt)
            
        except Exception as e:
            logger.error(f"[Context Relevance] è¯„ä¼°å¤±è´¥: {e}")
            return 5.0
    
    def compare_answers(
        self,
        query: str,
        answer1: str,
        answer2: str,
        technique1: str,
        technique2: str
    ) -> Dict[str, Any]:
        """
        å¯¹æ¯”ä¸¤ä¸ªç­”æ¡ˆçš„è´¨é‡
        
        Args:
            query: æŸ¥è¯¢
            answer1: ç­”æ¡ˆ1
            answer2: ç­”æ¡ˆ2
            technique1: æŠ€æœ¯1åç§°
            technique2: æŠ€æœ¯2åç§°
            
        Returns:
            å¯¹æ¯”ç»“æœ
        """
        logger.info(f"[AutoEvaluator] å¯¹æ¯”ç­”æ¡ˆ: {technique1} vs {technique2}")
        
        try:
            system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„ç­”æ¡ˆè´¨é‡å¯¹æ¯”ä¸“å®¶ã€‚è¯·å¯¹æ¯”ä¸¤ä¸ªç­”æ¡ˆçš„è´¨é‡ã€‚

è¯„åˆ†æ ‡å‡†ï¼ˆ1-10åˆ†ï¼‰ï¼š
- 10åˆ†ï¼šæ˜¾è‘—ä¼˜äºå¯¹æ–¹
- 7-9åˆ†ï¼šæ˜æ˜¾ä¼˜äºå¯¹æ–¹
- 5-6åˆ†ï¼šç•¥ä¼˜äºå¯¹æ–¹
- 4åˆ†ï¼šåŸºæœ¬ç›¸å½“
- 1-3åˆ†ï¼šä¸å¦‚å¯¹æ–¹

è¯·ä»…è¿”å›answer1çš„å¾—åˆ†ï¼ˆ1-10ï¼‰ã€‚"""
            
            user_prompt = f"""é—®é¢˜ï¼š{query}

ç­”æ¡ˆ1ï¼ˆ{technique1}ï¼‰ï¼š
{answer1}

ç­”æ¡ˆ2ï¼ˆ{technique2}ï¼‰ï¼š
{answer2}

ç­”æ¡ˆ1çš„å¾—åˆ†ï¼ˆ1-10ï¼‰ï¼š"""
            
            score = self._call_llm_for_score(system_prompt, user_prompt)
            
            # åˆ¤æ–­ä¼˜åŠ£
            if score >= 7:
                winner = technique1
                conclusion = f"{technique1} æ˜æ˜¾ä¼˜äº {technique2}"
            elif score >= 5:
                winner = technique1
                conclusion = f"{technique1} ç•¥ä¼˜äº {technique2}"
            elif score == 4:
                winner = "tie"
                conclusion = f"{technique1} ä¸ {technique2} è´¨é‡ç›¸å½“"
            else:
                winner = technique2
                conclusion = f"{technique2} ä¼˜äº {technique1}"
            
            return {
                "winner": winner,
                "score_difference": abs(score - 4),
                "conclusion": conclusion
            }
            
        except Exception as e:
            logger.error(f"[Compare Answers] å¯¹æ¯”å¤±è´¥: {e}")
            return {
                "winner": "tie",
                "score_difference": 0,
                "conclusion": "å¯¹æ¯”å¤±è´¥"
            }


# å•ä¾‹æ¨¡å¼
_evaluator_instance = None

def get_evaluator() -> AutoEvaluator:
    """è·å–è¯„ä¼°å™¨å•ä¾‹"""
    global _evaluator_instance
    if _evaluator_instance is None:
        _evaluator_instance = AutoEvaluator()
    return _evaluator_instance

